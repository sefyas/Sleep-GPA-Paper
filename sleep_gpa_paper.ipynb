{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registration information:\n",
    "\n",
    "- [The relationship between sleep and academic performance in first-year college students: a longitudinal, multi-university analysis](https://osf.io/5xngv?view_only=bf5d479c7e4a409dbd72b15165bfd560)\n",
    "- [The relationship between sleep and academic performance in first-year college students: a longitudinal, multi-university analysis (NetHealth continuation)](https://osf.io/x76b4?view_only=3d8f8c27841d4a5e90fce711c936c0ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import csv, json, sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "from shutil import copyfile\n",
    "from pandas.io.json import json_normalize\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, time, date\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from statistics import median\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.transforms import offset_copy\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Of UW-Specific Work**\n",
    "\n",
    "The raw UWEXP Fitbit data was provided as JSON files. We converted these to csv files in the same format as the Life@CMU datasets to run this data through the same pipeline. The following section converts the raw UWEXP data (given in \"Daily Sleep/\") to \"sleep_raw_data/\" and \"steps_raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all json files from raw data to json folder\n",
    "def copyAllJsonFilesFromRawToJsonFolder(phase):\n",
    "\n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "\n",
    "    raw_dir = os.path.join(uw, 'Daily Sleep')\n",
    "    csv_dir = os.path.join(uw, 'daily_sleep_raw_csv')\n",
    "    json_dir = os.path.join(uw, 'daily_sleep_raw_json')\n",
    "\n",
    "    # create directories if they don't yet exist\n",
    "    dir_list = [csv_dir, json_dir]\n",
    "    for d in dir_list:\n",
    "        if not os.path.exists(d): \n",
    "            os.makedirs(d)\n",
    "\n",
    "    for f in os.listdir(raw_dir):\n",
    "        if f[-4:] == 'json': \n",
    "            copyfile(os.path.join(raw_dir,f),\n",
    "                     os.path.join(json_dir,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "copyAllJsonFilesFromRawToJsonFolder('uw1')\n",
    "copyAllJsonFilesFromRawToJsonFolder('uw2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts one section of sleep episode data to a pandas dataframe\n",
    "# input: json_data[0]['sleep'][0] = one sleep episode data in JSON\n",
    "# output: dataframe with a dateTime = '%Y-%m-%d %H:%M:%S' and 'value' column\n",
    "def getDF(json_sleep_episode):\n",
    "    \n",
    "    # get the JSON values as dataframe\n",
    "    json_df = json_normalize(json_sleep_episode['minuteData']) # YSS: could just use pd.DataFrame(json_sleep_episode['minuteData'])\n",
    "    \n",
    "    # create a minute index with all the minutes in the sleep episode\n",
    "    start = json_sleep_episode['startTime']\n",
    "    end = json_sleep_episode['endTime']\n",
    "    TIME_INDEX = pd.date_range(start=start,end=end,freq='min')[:-1] # [:-1] b/c json excludes end point\n",
    "    \n",
    "    #YSS\n",
    "    if len(TIME_INDEX) != json_df.shape[0]:\n",
    "        # it is expected that there is a one-to-one matching between dateTime values in minuteData \n",
    "        # and the time index generated between start and end times\n",
    "        print('\\t\\tinvestigate mismatch in timing of sleep data')\n",
    "    \n",
    "    # build final dataframe\n",
    "    df = pd.DataFrame(index=TIME_INDEX)\n",
    "    df['dt'] = df.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['time'] = df['dt'].apply(lambda x: x.split()[1])\n",
    "    df = df.merge(json_df, how='left', left_on='time', right_on='dateTime')\n",
    "    df = df[['dt', 'value']]\n",
    "    df = df.rename(columns={'dt':'dateTime'})\n",
    "    \n",
    "    #YSS\n",
    "    if len(TIME_INDEX) != df.shape[0]:\n",
    "        # it is expected that there is a one-to-one matching between dateTime values in minuteData \n",
    "        # and the time index generated between start and end times\n",
    "        print('\\t\\tinvestigate issue in timestamping sleep data')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: json file\n",
    "# output: raw sleep dataframe\n",
    "def getParticipantSleepData(filename):\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # fix JSON formatting errors\n",
    "    count = data[0].count(\"}}\") - 1\n",
    "    data_val = '[' + data[0].replace('}}','}},', count) + ']'\n",
    "\n",
    "    # load JSON data\n",
    "    json_data = json.loads(data_val)\n",
    "    \n",
    "    # initialize empty dataframe\n",
    "    all_df = pd.DataFrame()\n",
    "    #all_df = [] #YSS\n",
    "    \n",
    "    # append all sleep episodes to empty dataframe\n",
    "    for idx, sleep_day in enumerate(json_data):\n",
    "        print('\\tprocessing sleep day {:03}...'.format(idx)) #YSS\n",
    "        for episode in sleep_day['sleep']:\n",
    "            episode_df = getDF(episode)\n",
    "            all_df = all_df.append(episode_df) #YSS use of concatenation is more efficient\n",
    "            \n",
    "    return all_df\n",
    "    #return pd.concat(all_df) #YSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAllJSONToCSV(phase):\n",
    "    \n",
    "    print('Beginning directory conversion...')\n",
    "\n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "\n",
    "    csv_dir = os.path.join(uw, 'daily_sleep_raw_csv') #YSS sleep_raw_csv -> daily_sleep_raw_csv\n",
    "    json_dir = os.path.join(uw, 'daily_sleep_raw_json')        \n",
    "        \n",
    "    for fn in tqdm(os.listdir(json_dir)):\n",
    "        print('converting {}...'.format(fn)) #YSS\n",
    "        df = getParticipantSleepData(os.path.join(json_dir, fn))\n",
    "        new_fn = os.path.join(csv_dir,fn.replace('.json', '.csv'))\n",
    "        df.to_csv(new_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "convertAllJSONToCSV('uw1')\n",
    "convertAllJSONToCSV('uw2')\n",
    "#YSS no further investigation was reported (good!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert steps csv to correct format\n",
    "# columns: datetime (%Y-%m-%d %H:%M:%S) and steps\n",
    "def processStepsFile(src, dest):\n",
    "    df = pd.read_csv(src)\n",
    "    df['datetime'] = df.date.str.cat(df.time, sep=\" \")\n",
    "    new_df = df[['datetime', 'steps']]\n",
    "    new_df.to_csv(dest, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entire steps raw folder\n",
    "def processAllSteps(phase):\n",
    "    \n",
    "    print('Beginning directory conversion...')\n",
    "    \n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "\n",
    "\n",
    "    steps_all_dir = os.path.join(uw, 'Daily Step Details')\n",
    "    steps_dir = os.path.join(uw, 'daily_steps_raw_data') #YSS steps_raw_data -> daily_steps_raw_data\n",
    "    \n",
    "    if not os.path.exists(steps_dir): \n",
    "        os.makedirs(steps_dir)    \n",
    "    \n",
    "    for fn in tqdm(os.listdir(steps_all_dir)):\n",
    "        if fn[-3:] == 'csv': \n",
    "            src = os.path.join(steps_all_dir, fn)\n",
    "            dest = os.path.join(steps_dir, fn)\n",
    "            processStepsFile(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "processAllSteps('uw1')\n",
    "processAllSteps('uw2')\n",
    "#YSS NOTE use of detailed step data is potentially problematic because of missing data otherwise available in daily step data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames files from 'PID001_step.csv' -> 'PID001.csv'\n",
    "def convertAllFilenamesToID(phase):\n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "    \n",
    "    sleep_dir = os.path.join(uw,'daily_sleep_raw_csv/') #YSS sleep_raw_data --> daily_sleep_raw_csv\n",
    "    steps_dir = os.path.join(uw,'daily_steps_raw_data/') #YSS steps_raw_data -> daily_steps_raw_data\n",
    "    \n",
    "    sleep_files = os.listdir(sleep_dir)\n",
    "    steps_files = os.listdir(steps_dir)\n",
    "    \n",
    "    #YSS\n",
    "    sleep_fns_unique = set([fn[:6] for fn in sleep_files])\n",
    "    if len(sleep_fns_unique) != len(sleep_files):\n",
    "        print('investigate split sleep data being overwritten')\n",
    "    step_fns_unique = set([fn[:6] for fn in steps_files])\n",
    "    if len(step_fns_unique) != len(steps_files):\n",
    "        print('investigate split step data being overwritten')\n",
    "    \n",
    "    for f in sleep_files:\n",
    "        src = os.path.join(sleep_dir,f)\n",
    "        new_f = f[:6] +'.csv'\n",
    "        dest = os.path.join(sleep_dir, new_f)\n",
    "        os.rename(src,dest)\n",
    "        \n",
    "    for f in steps_files:\n",
    "        src = os.path.join(steps_dir,f)\n",
    "        new_f = f[:6] +'.csv'\n",
    "        dest = os.path.join(steps_dir, new_f)\n",
    "        os.rename(src,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "convertAllFilenamesToID('uw1')\n",
    "convertAllFilenamesToID('uw2')\n",
    "#YSS PID136 data is split and the second one overwrited the first one in both sleep and step data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Of UW-Specific Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have multiple datasets, and getPath allows us to grab the path of whatever dataset we're working with. Each dataset folder assumes a similar underlying directory structure:\n",
    "* steps_raw_data\n",
    "* sleep_raw_data\n",
    "* sleep_steps_data\n",
    "* sleep_episodes\n",
    "* computed_features\n",
    "\n",
    "The steps_raw_data is optional, and for the NetHealth dataset, sleep_steps_data is missing steps. We don't need steps to help us compute sleep, rather steps is useful in differentiating missing data from all-nighters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPath(phase):\n",
    "    if phase == 'uw1':\n",
    "        path = 'Fitbit UWEXPI/'    \n",
    "    elif phase == 'uw2':\n",
    "        path = 'Fitbit UWEXPII/'\n",
    "    elif phase == 'lac1':\n",
    "        path = 'LifeAtCMU_Phase1/'\n",
    "    elif phase == 'lac2':\n",
    "        path = 'LifeAtCMU_Phase2/'\n",
    "    elif phase == 'nh':\n",
    "        path = 'NetHealth/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSleepStepsDir():\n",
    "    for d in ['uw1','uw2','lac1','lac2']:\n",
    "        new_d = os.path.join(getPath(d),'sleep_steps_data/')\n",
    "        if not os.path.exists(new_d): \n",
    "            os.makedirs(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "createSleepStepsDir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following combines all sleep and steps into a single combined dataframe for the given date range in the sleep_steps_data folder. This step is optional for these sleep analyses, and is done for NetHealth in the NetHealth cleaning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines all raw sleep and raw steps \n",
    "# input: \"uw1\", \"uw2\", \"lac1\", \"lac2\"\n",
    "def saveAllCombined(phase):\n",
    "\n",
    "    if phase == 'uw1':\n",
    "        TIME_INDEX = pd.date_range(start='1/1/2018',\n",
    "                                   end='6/13/2018',\n",
    "                                   freq='min')\n",
    "    elif phase == 'uw2': \n",
    "        TIME_INDEX = pd.date_range(start='4/1/2019',\n",
    "                                   end='6/14/2019',\n",
    "                                   freq='min')\n",
    "    elif phase == 'lac1':\n",
    "        TIME_INDEX = pd.date_range(start='1/16/2017',\n",
    "                                   end='5/15/2017',\n",
    "                                   freq='min')\n",
    "    elif phase == 'lac2':\n",
    "        TIME_INDEX = pd.date_range(start='1/16/2018',\n",
    "                                   end='5/15/2018',\n",
    "                                   freq='min')\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "        \n",
    "    path = getPath(phase)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sleep_dir = os.path.join(path,'daily_sleep_raw_csv/') #YSS sleep_raw_data --> daily_sleep_raw_data\n",
    "    steps_dir = os.path.join(path,'daily_steps_raw_data/') #YSS steps_raw_data -> daily_steps_raw_data\n",
    "    combined_dir = os.path.join(path,'sleep_steps_data/')\n",
    "\n",
    "    sleep_files = os.listdir(sleep_dir)\n",
    "    steps_files = os.listdir(steps_dir)\n",
    "    both = list(set(sleep_files) & set(steps_files))\n",
    "    both.sort() #YSS\n",
    "\n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    bad_ids = []\n",
    "    for idx in tqdm(range(len(both))):\n",
    "        \n",
    "        #print('check if {} exists ...'.format(os.path.join(combined_dir, both[idx])), end=' ') #YSS\n",
    "\n",
    "        if os.path.exists(os.path.join(combined_dir, both[idx])):\n",
    "            #print('YES; continue to the next file') #YSS\n",
    "            continue\n",
    "        else:\n",
    "            #print('NO; combine sleep and step files') #YSS\n",
    "            try:\n",
    "                df_sleep = pd.read_csv(os.path.join(sleep_dir, both[idx]))\n",
    "                df_steps = pd.read_csv(os.path.join(steps_dir, both[idx]))\n",
    "\n",
    "                #YSS - check if all rows have consistent second values\n",
    "                secs = df_sleep['dateTime'].apply(lambda time: time[-2:]).unique()\n",
    "                if len(secs) > 1:\n",
    "                    print('inconsistent sleep second values for {}: {}'.format(both[idx], list(secs)))\n",
    "                secs = df_steps['datetime'].apply(lambda time: time[-2:]).unique()\n",
    "                if len(secs) > 1:\n",
    "                    print('inconsistent step second values for {}: {}'.format(both[idx], list(secs)))\n",
    "                # NOTE should there be inconsistentsies, there is a chance that replacing seconds with 0 results in \n",
    "                #      duplicate timestamps\n",
    "                #print('finished 1st check')\n",
    "\n",
    "\n",
    "                # Convert times to datetime objects\n",
    "                df_sleep['dt_typed'] = df_sleep['dateTime'].apply(lambda time: datetime.strptime(time,DATE_FORMAT).replace(second=0))\n",
    "                df_steps['dt_typed'] = df_steps['datetime'].apply(lambda time: datetime.strptime(time,DATE_FORMAT).replace(second=0))\n",
    "                df_sleep = df_sleep.rename(columns={'value' : 'sleep_value'})\n",
    "\n",
    "                #YSS - check if any rows have date&time values that do not appear in TIME_INDEX\n",
    "                dt_index = df_sleep['dt_typed'].isin(TIME_INDEX)\n",
    "                if (~dt_index).sum() > 0:\n",
    "                    print('inconsistent sleep timestamps for {}: {}'.format(both[idx], df_sleep[~dt_index]['dt_typed']))\n",
    "                dt_index = df_steps['dt_typed'].isin(TIME_INDEX)\n",
    "                if (~dt_index).sum() > 0:\n",
    "                    print('inconsistent step timestamps for {}: {}'.format(both[idx], df_steps[~dt_index]['dt_typed']))\n",
    "                # NOTE if all timestamps exist in TIME_INDEX there should not be any duplicates\n",
    "                #print('finished 2nd check')\n",
    "\n",
    "                #YSS - check if there are duplicate timestamps\n",
    "                duplicates = df_sleep.groupby(by=['dt_typed']).size()\n",
    "                if duplicates[duplicates > 1].shape[0] > 0:\n",
    "                    print('duplicates in sleep dataframe for {}: {}'.format(both[idx], duplicates[duplicates > 1]))\n",
    "                duplicates = df_steps.groupby(by=['dt_typed']).size()\n",
    "                if duplicates[duplicates > 1].shape[0] > 0:\n",
    "                    print('duplicates in step dataframe for {}'.format(both[idx], duplicates[duplicates > 1]))\n",
    "                # NOTE duplciates in combination can only exist if there are duplciates in the original timestamps\n",
    "                #print('finished 3rd check')\n",
    "\n",
    "                df = pd.DataFrame(index=TIME_INDEX)\n",
    "                df = df.merge(df_sleep[['sleep_value','dt_typed']],how='left',left_index=True,right_on='dt_typed')\n",
    "                df = df.merge(df_steps[['steps','dt_typed']],how='left',left_on='dt_typed',right_on='dt_typed')\n",
    "\n",
    "                #YSS\n",
    "                duplicates = df.groupby(by=['dt_typed']).size()\n",
    "                if duplicates[duplicates > 1].shape[0] > 0:\n",
    "                    print('duplicates in combined dataframe for {}'.format(both[idx]))\n",
    "                    duplicates[duplicates > 1].to_csv('debug{}-combine_sleep_step.csv'.format(both[idx][:-4]))\n",
    "                #print('finished 4th check')\n",
    "\n",
    "                df = df.drop_duplicates(subset='dt_typed', keep='first') #YSS why should there be any duplicates?\n",
    "                df = df.set_index('dt_typed')\n",
    "\n",
    "                # lac1 only gives steps every 5 minutes, so we interpolate\n",
    "                if phase == 'lac1':\n",
    "                    df['steps'] = df['steps'].interpolate()\n",
    "\n",
    "                df.index.names = ['time']\n",
    "                df.sort_index(inplace=True)\n",
    "                df.to_csv(os.path.join(combined_dir, both[idx]))\n",
    "            except:\n",
    "                print('exception happened in processing', sys.exc_info()) #YSS\n",
    "                bad_ids.append(both[idx])\n",
    "    print(phase, bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN - uw1/2 both have a few empty participants\n",
    "# these will be ignored and their PIDs will be printed\n",
    "\n",
    "saveAllCombined('uw1')\n",
    "saveAllCombined('uw2')\n",
    "#YSS no duplicates for UW-I or UW-II\n",
    "saveAllCombined('lac1')\n",
    "#YSS no duplicates for CMU-I\n",
    "saveAllCombined('lac2')\n",
    "#YSS it seems that data appears twice under '202.csv', '224.csv', '245.csv', '277.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the number of minutes that separates two datetimes\n",
    "def diff_min(dt1,dt2):\n",
    "    \n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    td = datetime.strptime(dt1, DATE_FORMAT) - datetime.strptime(dt2, DATE_FORMAT)\n",
    "    return int(td.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the combined sleep-steps files, we then want to extract the sleep episodes. To do this, we have two parameters: MIN_CONSEC_NON_AWAKE and MAX_CONSEC_AWAKE.\n",
    "\n",
    "MIN_CONSEC_NON_AWAKE is the number of consecutive minutes that a student has to be asleep/restless for an episode to be recognized, and MAX_CONSEC_AWAKE is the number of consecutive awake minutes that have to be recognized for the termination of an episode. Any timepoint where steps are positive is considered awake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CONSEC_NON_AWAKE = 20\n",
    "MAX_CONSEC_AWAKE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectSleepEpisodes(phase, \n",
    "                            filename,\n",
    "                            min_consec_non_awake = MIN_CONSEC_NON_AWAKE,\n",
    "                            max_consec_awake = MAX_CONSEC_AWAKE):\n",
    "        \n",
    "    path = getPath(phase)\n",
    "\n",
    "    COMBINED_DIR = os.path.join(path, 'sleep_steps_data/')\n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')\n",
    "    \n",
    "    # Read combined data\n",
    "    filename = os.path.join(COMBINED_DIR, filename)\n",
    "    print('episodes for', filename) #YSS\n",
    "    combined_df = pd.read_csv(filename, index_col=0)\n",
    "    \n",
    "    #YSS\n",
    "    duplicates = combined_df.groupby(combined_df.index).size()\n",
    "    if duplicates[duplicates > 1].shape[0] > 0:\n",
    "        print('duplicates in combined dataframe for {}'.format(filename))\n",
    "        duplicates[duplicates > 1].to_csv('debug{}-episode_sleep_step.csv'.formt(filename[:-4]))\n",
    "    \n",
    "    # Define fields\n",
    "    columns = ['start_time', 'end_time', 'length', 'time_asleep', 'time_restless', 'time_awake']\n",
    "    \n",
    "    # Make dictionary\n",
    "    episode_dict = {} # start_time -> ['end_time', 'length', 'time_asleep', 'time_restless', 'time_awake']\n",
    "    \n",
    "    # tracks the last time point the subject was awake\n",
    "    last_awake = combined_df.index[0]\n",
    "    \n",
    "    # used for tracking when in or not in and episode\n",
    "    consec_non_awake = 0\n",
    "    consec_awake = 0\n",
    "    on_episode = False\n",
    "    start_index = None\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep='first')] #YSS what is this about duplicated   \n",
    "    \n",
    "    # iterate through the subject's combined dataframe\n",
    "    for index, row in combined_df.iterrows():\n",
    "        sleep = row['sleep_value']\n",
    "\n",
    "        # update counts\n",
    "        if math.isnan(sleep) or sleep == 3 or sleep == 0: # 3 = awake, 0 = missing in lac1\n",
    "            consec_awake += 1\n",
    "            consec_non_awake = 0\n",
    "            last_awake = index\n",
    "        elif sleep == 1: # 1 = asleep\n",
    "            consec_awake = 0\n",
    "            consec_non_awake += 1\n",
    "        elif sleep == 2: # 2 = restless\n",
    "            consec_awake = 0\n",
    "            consec_non_awake += 1\n",
    "        else:\n",
    "            raise Exception('sleep value is not NaN, 1, 2, or 3')\n",
    "\n",
    "        # update episode status\n",
    "        # starting new episode\n",
    "        if not on_episode and consec_non_awake >= min_consec_non_awake:\n",
    "            on_episode = True\n",
    "            start_index = last_awake\n",
    "            start_loc = combined_df.index.get_loc(index)\n",
    "        # ending episode\n",
    "        elif on_episode and consec_awake >= max_consec_awake:\n",
    "            index_loc = combined_df.index.get_loc(index)\n",
    "            end_index = combined_df.index[index_loc-max_consec_awake+1]\n",
    "            length = diff_min(end_index,start_index)\n",
    "            temp_df = combined_df.iloc[start_loc:index_loc]\n",
    "            time_awake = len(temp_df[temp_df['sleep_value']==3])\n",
    "            time_restless = len(temp_df[temp_df['sleep_value']==2])\n",
    "            time_asleep = length - time_restless - time_awake\n",
    "            episode_dict[start_index] = [end_index, length, time_asleep, time_restless, time_awake]\n",
    "            start_index = index\n",
    "            on_episode = False\n",
    "\n",
    "    # Make dataframe from dictionary\n",
    "    episode_df = pd.DataFrame.from_dict(episode_dict, orient='index', columns=columns[1:])\n",
    "    episode_df.index.name = columns[0]\n",
    "    \n",
    "    return episode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIN_CONSEC_NON_AWAKE = 4\n",
    "#MAX_CONSEC_AWAKE = 2\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([1, 1, 3, 1, 1, 1, 3, 1, 1], index=combined_df.iloc[:9].index) # no episode\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([3, 3, 1, 1, 1, 1, 3, 1, 1], index=combined_df.iloc[:9].index) # no episode\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([1, 3, 1, 1, 1, 1, 3, 3, 1], index=combined_df.iloc[:9].index)\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([3, 1, 1, 1, 1, 3, 1, 3, 3], index=combined_df.iloc[:9].index)\n",
    "#YSS issues in episode calculations (all minor):\n",
    "# - sensitive to awake oscillation at the start of an episode but not in the middle\n",
    "# - neither start_time nor end_time are inclusive \n",
    "#   (the episode actually starts after start_time and ends before end_time)\n",
    "# - actual length of time is smaller by one minute\n",
    "# - cannot find episodes if there are not enough awake minutes at the end of the timeseries; \n",
    "#   can be fixed by padding MAX_CONSEC_AWAKE awake minutes at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAllEpisodesForParams(phase,\n",
    "                                min_consec_non_awake, \n",
    "                                max_consec_awake):\n",
    "\n",
    "    path = getPath(phase)\n",
    "\n",
    "    COMBINED_DIR = os.path.join(path, 'sleep_steps_data/')\n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')    \n",
    "        \n",
    "    if not os.path.exists(EPISODES_DIR): \n",
    "        os.makedirs(EPISODES_DIR) \n",
    "    \n",
    "    for src in tqdm(os.listdir(COMBINED_DIR)):\n",
    "\n",
    "        dest = 'EPI_' + src.split('.')[0] + '.csv'\n",
    "\n",
    "        dest = os.path.join(EPISODES_DIR,dest)\n",
    "\n",
    "        if not os.path.exists(dest):    # remove to writeover existing files\n",
    "            df = getSubjectSleepEpisodes(\n",
    "                     phase,\n",
    "                     src,\n",
    "                     min_consec_non_awake,\n",
    "                     max_consec_awake)\n",
    "            df.to_csv(dest)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN - took 3-5 hours on my personal computer \n",
    "# for phase in ['uw1', 'uw2', 'lac1', 'lac2', 'nh']:\n",
    "#     computeAllEpisodesForParams(phase,\n",
    "#                             MIN_CONSEC_NON_AWAKE,\n",
    "#                             MAX_CONSEC_AWAKE)\n",
    "computeAllEpisodesForParams('uw1', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "computeAllEpisodesForParams('uw2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "computeAllEpisodesForParams('lac1', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "computeAllEpisodesForParams('lac2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "#computeAllEpisodesForParams('nh', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeAllEpisodesForParams('uw2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeAllEpisodesForParams('lac1', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeAllEpisodesForParams('lac2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've extracted the sleep episodes (which are in the sleep_episodes/ folder under a subdirectory titled \"MIN_CONSEC_NONAWAKE\"\"MAX_CONSEC_AWAKE\"), we then have to compute the sleep features of interest.\n",
    "\n",
    "To do so, we need to convert the bedtime and waketime into minutes after a given zero (e.g. minutes after 6 pm) since time is modulo 24 hr. For bedtime, students generally go to bed after 6 pm, and for waketime, students generally wake up after 4 am (these numbers were not chosen formally). Thus, we set BED_ZERO = 18 (i.e. 6 pm), and WAKE_ZERO = 4 (i.e. 4 am), and convert bedtime and waketime to the number of minutes after these respective zeros.\n",
    "\n",
    "Additionally, we identified the main sleep episode of Day n as the longest sleep episode which started between noon of Day n and noon of Day (n+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BED_ZERO = 18\n",
    "WAKE_ZERO = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the main episode date for given time\n",
    "def getWindow(time):\n",
    "    if time.hour < 12:\n",
    "        time = time - timedelta(days=1)\n",
    "    return time.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpisodeData(phase,\n",
    "                   filename): # ex: 'PID001.csv'\n",
    "    \n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    path = getPath(phase)\n",
    "    \n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')\n",
    "    file = os.path.join(EPISODES_DIR, 'EPI_'+ filename.split('.')[0] + '.csv')\n",
    "    \n",
    "    df = pd.read_csv(file, header=0) #YSS consider using pd.read_csv(file, header=0, parse_dates=['start_time', 'end_time'])\n",
    "    \n",
    "    df['start_time'] = df['start_time'].apply(lambda x: datetime.strptime(x,DATE_FORMAT)) #YSS not needed if using parse_dates in read_csv\n",
    "    df['end_time'] = df['end_time'].apply(lambda x: datetime.strptime(x,DATE_FORMAT)) #YSS not needed if using parse_dates in read_csv\n",
    "    \n",
    "    # define a main sleep episode as the longest sleep episode\n",
    "    # in sleep window\n",
    "    df['subject_id'] = filename.split('.')[0]\n",
    "    df['main_episode_of'] = df['start_time'].apply(lambda x: getWindow(x))\n",
    "    #df = df.sort_values(by='length', ascending=False) #YSS not needed\n",
    "    #df = df.sort_values(by='main_episode_of') #YSS not needed\n",
    "    \n",
    "    #YSS\n",
    "    df.index.name = 'inds'\n",
    "    main_index = df.reset_index().groupby(by=['main_episode_of']).apply(lambda x: \n",
    "                                                                        x.sort_values(by=['length'], \n",
    "                                                                                      ascending=False).iloc[0]['ind'])\n",
    "    df['main_sleep'] = False\n",
    "    df.loc[df.index.isin(main_index), 'main_sleep'] = True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainEpisodeData(phase, filename):\n",
    "    \n",
    "    df = getEpisodeData(phase, filename)\n",
    "    df = df.sort_values(by='length', ascending=False)\n",
    "    df = df.drop_duplicates(subset='main_episode_of',keep='first')\n",
    "    df = df.sort_values(by='main_episode_of') \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeToMin(time, zero_hour):\n",
    "    \n",
    "    hour = time.hour\n",
    "    minute = time.minute\n",
    "    \n",
    "    if hour < zero_hour: #YSS so if bed_time is 5pm with BED_ZERO as 6pm you want to return \n",
    "        hour += 24\n",
    "        \n",
    "    return (hour-zero_hour)*60 + minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compute various sleep features of interest for a given time window.\n",
    "\n",
    "MSSD refers to mean successive squared difference. This is a measure of variability that also takes into account the temporal nature of the data. For instance, the MSSD of [3,4,10] is ((4-3)^2 + (10-4)^2)/2 = 37/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainEpisodeMSSD(main_episode_df):\n",
    "    \n",
    "    main_episode_df = main_episode_df.sort_values(by='main_episode_of')\n",
    "    \n",
    "    main_episode_df['bedtime'] = main_episode_df['start_time'].apply(lambda x: timeToMin(x,BED_ZERO))\n",
    "    main_episode_df['waketime'] = main_episode_df['end_time'].apply(lambda x: timeToMin(x,WAKE_ZERO))\n",
    "    main_episode_df['midpoint_sleep'] = (main_episode_df['waketime'] + main_episode_df['bedtime']) / 2.0\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    bt_total = []\n",
    "    wt_total = []\n",
    "    mp_total = []\n",
    "    \n",
    "    for i, idx1 in enumerate(main_episode_df.index): #YSS this does not handle data dicontinuity (e.g. missing days)\n",
    "        row1 = main_episode_df.loc[idx1]\n",
    "        \n",
    "        if i < len(main_episode_df.index)-1:\n",
    "            idx2 = main_episode_df.index[i+1]\n",
    "            row2 = main_episode_df.loc[idx2]\n",
    "        else:\n",
    "            row2 = None\n",
    "        \n",
    "        if row2 is not None:\n",
    "            count += 1\n",
    "            bt_total.append((row2['bedtime']-row1['bedtime'])**2)\n",
    "            wt_total.append((row2['waketime']-row1['waketime'])**2)\n",
    "            mp_total.append((row2['midpoint_sleep']-row1['midpoint_sleep'])**2)\n",
    "        \n",
    "    \n",
    "    if count == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    bt_mssd = np.array(bt_total).mean() / (float(count) * 3600) #YR: why deviding by float(count) SP: there should not be\n",
    "    wt_mssd = np.array(wt_total).mean() / (float(count) * 3600)\n",
    "    mp_mssd = np.array(mp_total).mean() / (float(count) * 3600)\n",
    "        \n",
    "    bt_mssd_median = median(bt_total)\n",
    "    wt_mssd_median = median(wt_total)\n",
    "    mp_mssd_median = median(mp_total)\n",
    "        \n",
    "    return bt_mssd, wt_mssd, mp_mssd, bt_mssd_median, wt_mssd_median, mp_mssd_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given time window, computeSummaryStats computes all the sleep features for a single participant, while computeAllSummaryStats computes all the sleep features for **all** participants for the given time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_window and end_window are date objects\n",
    "# extracts start to end inclusive\n",
    "def computeSummaryStats(phase, \n",
    "                        filename, # ex: 'PID001.csv'\n",
    "                        start_window, \n",
    "                        end_window):\n",
    "    \n",
    "        \n",
    "        DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "        \n",
    "        summary = dict()\n",
    "        \n",
    "        subject_id = filename.split('.')[0]\n",
    "        \n",
    "        summary['subject_id'] = subject_id\n",
    "        \n",
    " \n",
    "        all_df = getEpisodeData(phase, subject_id)\n",
    "        main_df = getMainEpisodeData(phase, subject_id)\n",
    "\n",
    "        start = pd.Timestamp(start_window)\n",
    "        end = pd.Timestamp(end_window)\n",
    "        main_df = main_df[main_df['main_episode_of'].between(start,end)]\n",
    "        \n",
    "        #all_df['start_time'] = all_df['start_time'].apply(lambda x: datetime.strptime(str(x),DATE_FORMAT)) #YSS this is already done in getEpisodeData\n",
    "        #all_df['main_episode_of'] = all_df['start_time'].apply(lambda x: getWindow(x)) #YSS this s already done in getEpisodeData\n",
    "        all_df = all_df[all_df['main_episode_of'].between(start,end)] \n",
    "        \n",
    "        nap_df = all_df[~all_df.start_time.isin(main_df.start_time)]\n",
    "        \n",
    "        #YSS\n",
    "        if all_df.shape[0] != (main_df.shape[0] + nap_df.shape[0]):\n",
    "            print('investigate construction of nap_df in {} (inequality)'.format(filename))\n",
    "        if all_df[~all_df['main_leep']].shape[0] != nap_df.shape[0]:\n",
    "            print('investigate construction of nap_df in {} (alternative construction)'.format(filename))\n",
    "        \n",
    "        summary['num_naps'] = len(nap_df)\n",
    "        summary['avg_nap_length'] = nap_df['length'].mean()\n",
    "        \n",
    "        #YSS\n",
    "        if all_df.shape[0] > 0:\n",
    "            if all_df[~all_df['main_leep']]['length'].mean() != summary['avg_nap_length']:\n",
    "                print('investigate construction of nap_df {} (alternative construction - mean)'.format(filename))\n",
    "            if all_df[~all_df['main_leep']]['length'].sum() != nap_df['length'].sum():\n",
    "                print('investigate construction of nap_df {} (alternative construction - sum)'.format(filename))\n",
    "            if ((all_df.groupby(all_df.set_index('start_time').index.dayofyear)['length'].sum()).mean() - \n",
    "                all_df['length'].sum() / len(all_df['main_episode_of'].unique())).abs() > 0.01:\n",
    "                print('investigate construction of nap_df {} (alternative construction - sum)'.format(filename))\n",
    "        \n",
    "        try:\n",
    "            summary['frac_napped_of_total_sleep'] = nap_df['length'].sum() / all_df['length'].sum()\n",
    "            summary['avg_24_hr_sleep'] = all_df['length'].sum() / len(all_df['main_episode_of'].unique()) #YSS issue: all_df should first be filtered on days with main_sleep; the alternative is to groupby dayofyear, sum on each day, and the get the mean of the result\n",
    "            summary['frac_sleep_episodes_as_naps'] = len(nap_df) / len(all_df)\n",
    "            \n",
    "        except: # all_df is empty\n",
    "            #YSS suggest filling in all the other feilds and returning summary\n",
    "            summary['frac_napped_of_total_sleep'] = np.nan\n",
    "            summary['avg_24_hr_sleep'] = np.nan\n",
    "            summary['frac_sleep_episodes_as_naps'] = np.nan\n",
    "        \n",
    "        summary['frac_nights_with_data'] = len(main_df) / ((end_window-start_window).days + 1)\n",
    " \n",
    "\n",
    "        if len(main_df) == 0:\n",
    "            #YSS this is not consistent with try-catch above\n",
    "            return None\n",
    "\n",
    "        \n",
    "        main_df['bedtime'] = main_df['start_time'].apply(lambda x: timeToMin(x,BED_ZERO)) #YSS I do not think it matters to change the reference point\n",
    "        main_df['waketime'] = main_df['end_time'].apply(lambda x: timeToMin(x,WAKE_ZERO)) #YSS I do not think it matters to change the reference point\n",
    "        main_df['midpoint_sleep'] = (main_df['waketime'] + main_df['bedtime']) / 2.0 #YSS waketime and bedtime are calculated wrt different reference points; what does it mean to find their average? what reference point to use to interpret this average?\n",
    "        #YSS alterantive approain find mid_time based on 'start_time' and length of the main episode then similarly remap it wrt a reference (e.g. BED_ZERO)\n",
    "\n",
    "            \n",
    "        main_df['proportion_awake'] = main_df['time_awake'] / main_df['length']\n",
    "        main_df['proportion_restless'] = main_df['time_restless'] / main_df['length']\n",
    "        main_df['weekday'] = main_df['main_episode_of'].apply(lambda x: x.weekday())\n",
    "        \n",
    "        # 4 = friday night, 5 = saturday night\n",
    "        main_df['is_weekend'] = main_df['weekday'].apply(lambda x: x in [4,5])\n",
    "        \n",
    "        # bedtime measures\n",
    "        summary['bedtime'] = main_df['bedtime'].mean()\n",
    "        summary['bedtime_std'] = main_df['bedtime'].std()\n",
    "    \n",
    "        # waketime measures\n",
    "        summary['waketime'] = main_df['waketime'].mean()\n",
    "        \n",
    "        # midpoint sleep measures\n",
    "        summary['midpoint_sleep'] = main_df['midpoint_sleep'].mean()\n",
    "\n",
    "        # weekday measures\n",
    "        try:  \n",
    "            summary['bedtime_weekday'] = main_df.groupby('is_weekend')['bedtime'].mean()[False]        \n",
    "            summary['waketime_weekday'] = main_df.groupby('is_weekend')['waketime'].mean()[False]\n",
    "            summary['midpoint_sleep_weekday'] = main_df.groupby('is_weekend')['midpoint_sleep'].mean()[False]            \n",
    "        except KeyError: #YSS if an empty summary (all fields are np.nan) is returned in the first try-catch this won't be needed\n",
    "            summary['bedtime_weekday'] = np.nan     \n",
    "            summary['waketime_weekday'] = np.nan\n",
    "            summary['midpoint_sleep_weekday'] = np.nan                \n",
    "        \n",
    "        # weekend measures\n",
    "        try:\n",
    "            summary['bedtime_weekend'] = main_df.groupby('is_weekend')['bedtime'].mean()[True]  \n",
    "            summary['waketime_weekend'] = main_df.groupby('is_weekend')['waketime'].mean()[True]      \n",
    "            summary['midpoint_sleep_weekend'] = main_df.groupby('is_weekend')['midpoint_sleep'].mean()[True]      \n",
    "        except KeyError: #YSS if an empty summary (all fields are np.nan) is returned in the first try-catch this won't be needed\n",
    "            summary['bedtime_weekend'] = np.nan\n",
    "            summary['waketime_weekend'] = np.nan\n",
    "            summary['midpoint_sleep_weekend'] = np.nan\n",
    "\n",
    "            \n",
    "        try:\n",
    "            summary['social_jetlag'] = summary['bedtime_weekend'] - summary['bedtime_weekday'] #YSS this is incorrect midpoint sleep is used for calculating social jetlag\n",
    "        except:\n",
    "            summary['social_jetlag'] = np.nan\n",
    "            \n",
    "        summary['time_in_bed'] = main_df['length'].mean()\n",
    "                \n",
    "        bt_mssd, wt_mssd, mp_mssd, bt_mssd_median, wt_mssd_median, mp_mssd_median = getMainEpisodeMSSD(main_df)\n",
    "        \n",
    "        summary['bedtime_mssd'] = bt_mssd\n",
    "        summary['waketime_mssd'] = wt_mssd\n",
    "        summary['midpoint_sleep_mssd'] = mp_mssd\n",
    "        \n",
    "        summary['bedtime_mssd_median'] = bt_mssd_median\n",
    "        summary['waketime_mssd_median'] = wt_mssd_median\n",
    "        summary['midpoint_sleep_mssd_median'] = mp_mssd_median\n",
    "        \n",
    "        summary['WASO_fraction'] = main_df['proportion_awake'].mean() #YSS what is WASO?\n",
    "        summary['restless_fraction'] = main_df['proportion_restless'].mean()\n",
    "        summary['TST'] = main_df['time_asleep'].mean()\n",
    "        summary['TST_std'] = main_df['time_asleep'].std()\n",
    "        \n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>length</th>\n",
       "      <th>time_asleep</th>\n",
       "      <th>time_restless</th>\n",
       "      <th>time_awake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2018-06-03 02:19:00</td>\n",
       "      <td>2018-06-03 08:39:00</td>\n",
       "      <td>380</td>\n",
       "      <td>364</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2018-06-03 12:47:00</td>\n",
       "      <td>2018-06-03 21:02:00</td>\n",
       "      <td>495</td>\n",
       "      <td>465</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2018-06-04 11:54:00</td>\n",
       "      <td>2018-06-04 19:34:00</td>\n",
       "      <td>460</td>\n",
       "      <td>425</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2018-06-05 11:21:00</td>\n",
       "      <td>2018-06-05 19:49:00</td>\n",
       "      <td>508</td>\n",
       "      <td>480</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2018-06-06 15:17:00</td>\n",
       "      <td>2018-06-06 19:58:00</td>\n",
       "      <td>281</td>\n",
       "      <td>264</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2018-06-07 10:20:00</td>\n",
       "      <td>2018-06-07 14:29:00</td>\n",
       "      <td>249</td>\n",
       "      <td>223</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2018-06-08 11:08:00</td>\n",
       "      <td>2018-06-08 17:50:00</td>\n",
       "      <td>402</td>\n",
       "      <td>377</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2018-06-09 18:53:00</td>\n",
       "      <td>2018-06-09 20:34:00</td>\n",
       "      <td>101</td>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>2018-06-11 02:04:00</td>\n",
       "      <td>2018-06-11 10:10:00</td>\n",
       "      <td>486</td>\n",
       "      <td>462</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>2018-06-12 02:17:00</td>\n",
       "      <td>2018-06-12 10:42:00</td>\n",
       "      <td>505</td>\n",
       "      <td>475</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             start_time            end_time  length  time_asleep  \\\n",
       "154 2018-06-03 02:19:00 2018-06-03 08:39:00     380          364   \n",
       "155 2018-06-03 12:47:00 2018-06-03 21:02:00     495          465   \n",
       "156 2018-06-04 11:54:00 2018-06-04 19:34:00     460          425   \n",
       "157 2018-06-05 11:21:00 2018-06-05 19:49:00     508          480   \n",
       "158 2018-06-06 15:17:00 2018-06-06 19:58:00     281          264   \n",
       "159 2018-06-07 10:20:00 2018-06-07 14:29:00     249          223   \n",
       "160 2018-06-08 11:08:00 2018-06-08 17:50:00     402          377   \n",
       "161 2018-06-09 18:53:00 2018-06-09 20:34:00     101           96   \n",
       "162 2018-06-11 02:04:00 2018-06-11 10:10:00     486          462   \n",
       "163 2018-06-12 02:17:00 2018-06-12 10:42:00     505          475   \n",
       "\n",
       "     time_restless  time_awake  \n",
       "154             16           0  \n",
       "155             30           0  \n",
       "156             35           0  \n",
       "157             26           2  \n",
       "158             17           0  \n",
       "159             24           2  \n",
       "160             22           3  \n",
       "161              5           0  \n",
       "162             24           0  \n",
       "163             30           0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/Users/yasaman/UWEXP/cmu-sleep-gpa/Fitbit UWEXPI/sleep_episodes/EPI_PID155.csv'\n",
    "df = pd.read_csv(file, header=0, parse_dates=['start_time', 'end_time'])\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1277"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeToMin(df['start_time'].iloc[158], BED_ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 124 184.0, "
     ]
    }
   ],
   "source": [
    "btime = datetime.strptime('2018-06-11 22:04:00', '%Y-%m-%d %H:%M:%S')\n",
    "wtime = datetime.strptime('2018-06-12 06:04:00', '%Y-%m-%d %H:%M:%S')\n",
    "mtime = datetime.strptime('2018-06-12 02:04:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "bedtime = timeToMin(btime, BED_ZERO)\n",
    "waketime = timeToMin(wtime, WAKE_ZERO)\n",
    "midpoint = (bedtime + waketime) / 2.0\n",
    "print(bedtime, waketime, midpoint, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeToMin(mtime, BED_ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeToMin(mtime, WAKE_ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeToMin(mtime, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_window and end_window are date objects\n",
    "def computeAllSummaryStats(phase,\n",
    "                           start_window, \n",
    "                           end_window,\n",
    "                           overwrite=False):\n",
    "    \n",
    "    path = getPath(phase)\n",
    "\n",
    "    FEATURES_FOLDER = os.path.join(path, 'computed_features/')\n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')\n",
    "    feature_file = str(start_window) + '_' + str(end_window) + '.csv'       \n",
    "    feature_file = os.path.join(FEATURES_FOLDER,feature_file)    \n",
    "   \n",
    "    if os.path.exists(feature_file) and not overwrite:\n",
    "        return 'Already written'\n",
    "        \n",
    "    ids = os.listdir(os.path.join(path, 'sleep_steps_data/'))\n",
    "\n",
    "    \n",
    "    with open(feature_file, 'w') as f:\n",
    "    \n",
    "        header = ['subject_id',\n",
    "                  'num_naps',\n",
    "                  'avg_nap_length',\n",
    "                  'frac_napped_of_total_sleep',\n",
    "                  'avg_24_hr_sleep',\n",
    "                  'frac_sleep_episodes_as_naps',\n",
    "                  'frac_nights_with_data',\n",
    "                  'bedtime',\n",
    "                  'waketime',\n",
    "                  'midpoint_sleep',\n",
    "                  'time_in_bed',\n",
    "                  'bedtime_mssd',\n",
    "                  'bedtime_mssd_median',\n",
    "                  'bedtime_std',\n",
    "                  'bedtime_weekend',\n",
    "                  'bedtime_weekday',\n",
    "                  'waketime_mssd',\n",
    "                  'waketime_mssd_median',\n",
    "                  'waketime_weekend',\n",
    "                  'waketime_weekday',\n",
    "                  'midpoint_sleep_mssd',\n",
    "                  'midpoint_sleep_mssd_median',\n",
    "                  'midpoint_sleep_weekend',\n",
    "                  'midpoint_sleep_weekday',\n",
    "                  'WASO_fraction',\n",
    "                  'restless_fraction',\n",
    "                  'social_jetlag',\n",
    "                  'TST',\n",
    "                  'TST_std']\n",
    "        \n",
    "        w = csv.DictWriter(f, header)\n",
    "        w.writeheader()\n",
    "        \n",
    "        for filename in ids:\n",
    "    \n",
    "            summary = computeSummaryStats(phase, \n",
    "                                          filename,\n",
    "                                          start_window, \n",
    "                                          end_window)\n",
    "                \n",
    "            if not summary:\n",
    "                summary = dict()\n",
    "                for val in header:\n",
    "                    summary[val] = np.nan\n",
    "                summary['subject_id'] = filename.split('.')[0]\n",
    "                summary['frac_nights_with_data'] = 0\n",
    "                \n",
    "            w.writerow(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "# for d in ['uw1','uw2','lac1','lac2', 'nh']:\n",
    "#     new_d = os.path.join(getPath(d),'computed_features/')\n",
    "#     if not os.path.exists(new_d): \n",
    "#         os.makedirs(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSION_PERIODS = [('uw1',date(2018,4,1),date(2018,4,28)),\n",
    "                      ('uw2',date(2019,4,7),date(2019,5,4)),\n",
    "                      ('lac1',date(2017,2,7),date(2017,3,7)),\n",
    "                      ('lac2',date(2018,2,7),date(2018,3,7)),\n",
    "                      ('nh', date(2016,2,4), date(2016,3,4))]\n",
    "\n",
    "PREDICTORS = ['bedtime_mssd',\n",
    "              'TST',\n",
    "              'midpoint_sleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAllFeatureWindows():\n",
    "    for period in REGRESSION_PERIODS:\n",
    "\n",
    "        phase = period[0]\n",
    "        start_window = period[1]\n",
    "        end_window = period[2]\n",
    "\n",
    "        computeAllSummaryStats(phase, start_window, end_window, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStats(phase, start, end):\n",
    "\n",
    "    start_window = start\n",
    "    end_window = end\n",
    "    \n",
    "    path = getPath(phase)\n",
    "            \n",
    "    # get entire semester participant data   \n",
    "    filename = str(start_window) + '_' + str(end_window) + '.csv'\n",
    "    filename = os.path.join(path, 'computed_features/', filename)\n",
    "    summary_stats_df = pd.read_csv(filename)\n",
    "    summary_stats_df.set_index('subject_id', inplace=True)\n",
    "    \n",
    "    return summary_stats_df[summary_stats_df['frac_nights_with_data']>=.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to extract first-year student label from EMA for lac2\n",
    "def getLAC2Freshmen():\n",
    "    phase = 'lac2'\n",
    "    path = os.path.join('GPA_data/raw_gpa/',\n",
    "                        phase + '_pre_post_ema.csv')\n",
    "    df = pd.read_csv(path, index_col='ID', low_memory=False)\n",
    "    return list(df[df['IRA_YearOfStudy']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGPAData(phase):\n",
    "    path = os.path.join('GPA_data/cleaned_gpa/', \n",
    "                        phase + '_freshmen_gpa.csv')\n",
    "    return pd.read_csv(path, index_col='subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all predictors of interest, GPA features, no nan rows,\n",
    "# and participants with >=20% fraction of nights data \n",
    "def getRegDF(phase, start, end, predictors, control=None, thresh=0.2):\n",
    "        \n",
    "        # compute summary statistics\n",
    "        computeAllSummaryStats(phase, start, end, overwrite=True) # ADD overwrite=True\n",
    "        stats_df = getSummaryStats(phase, start, end)\n",
    "        \n",
    "        # filter by freshmen\n",
    "        if phase == 'lac2':\n",
    "            stats_df = stats_df[stats_df.index.isin(getLAC2Freshmen())]\n",
    "\n",
    "        # filter by completeness threshold\n",
    "        stats_df = stats_df[stats_df['frac_nights_with_data'] >= thresh]\n",
    "\n",
    "        # get GPA data\n",
    "        gpa_df = getGPAData(phase)\n",
    "        \n",
    "        # combine summary statistics with GPA data\n",
    "        combined_df = stats_df.merge(gpa_df, on='subject_id', how='outer')\n",
    "\n",
    "        # choose only columns of variables of interest\n",
    "        columns = predictors + ['cum_gpa', 'term_gpa', 'frac_nights_with_data']\n",
    "                \n",
    "        if control:\n",
    "            columns += [control]        \n",
    "        \n",
    "        combined_df = combined_df[columns].dropna()\n",
    "        \n",
    "        return combined_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveRegressionDF():\n",
    "\n",
    "    dfs = []\n",
    "    for period in REGRESSION_PERIODS:\n",
    "        df = getRegDF(period[0], period[1], period[2], PREDICTORS, thresh=0.2)\n",
    "        df['cohort'] = period[0]\n",
    "        dfs.append(df)\n",
    "    all_reg_dfs = pd.concat(dfs)\n",
    "    all_reg_dfs.to_csv('regression_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "# saveRegressionDF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
