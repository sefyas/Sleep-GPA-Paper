{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registration information:\n",
    "\n",
    "- [The relationship between sleep and academic performance in first-year college students: a longitudinal, multi-university analysis](https://osf.io/5xngv?view_only=bf5d479c7e4a409dbd72b15165bfd560)\n",
    "- [The relationship between sleep and academic performance in first-year college students: a longitudinal, multi-university analysis (NetHealth continuation)](https://osf.io/x76b4?view_only=3d8f8c27841d4a5e90fce711c936c0ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import csv, json, sys\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "from shutil import copyfile\n",
    "from pandas.io.json import json_normalize\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, time, date\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from statistics import median\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.transforms import offset_copy\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Of UW-Specific Work**\n",
    "\n",
    "The raw UWEXP Fitbit data was provided as JSON files. We converted these to csv files in the same format as the Life@CMU datasets to run this data through the same pipeline. The following section converts the raw UWEXP data (given in \"Daily Sleep/\") to \"sleep_raw_data/\" and \"steps_raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all json files from raw data to json folder\n",
    "def copyAllJsonFilesFromRawToJsonFolder(phase):\n",
    "\n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "\n",
    "    raw_dir = os.path.join(uw, 'Daily Sleep')\n",
    "    csv_dir = os.path.join(uw, 'daily_sleep_raw_csv')\n",
    "    json_dir = os.path.join(uw, 'daily_sleep_raw_json')\n",
    "\n",
    "    # create directories if they don't yet exist\n",
    "    dir_list = [csv_dir, json_dir]\n",
    "    for d in dir_list:\n",
    "        if not os.path.exists(d): \n",
    "            os.makedirs(d)\n",
    "\n",
    "    for f in os.listdir(raw_dir):\n",
    "        if f[-4:] == 'json': \n",
    "            copyfile(os.path.join(raw_dir,f),\n",
    "                     os.path.join(json_dir,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "copyAllJsonFilesFromRawToJsonFolder('uw1')\n",
    "copyAllJsonFilesFromRawToJsonFolder('uw2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts one section of sleep episode data to a pandas dataframe\n",
    "# input: json_data[0]['sleep'][0] = one sleep episode data in JSON\n",
    "# output: dataframe with a dateTime = '%Y-%m-%d %H:%M:%S' and 'value' column\n",
    "def getDF(json_sleep_episode):\n",
    "    \n",
    "    # get the JSON values as dataframe\n",
    "    json_df = json_normalize(json_sleep_episode['minuteData']) # YSS: could just use pd.DataFrame(json_sleep_episode['minuteData'])\n",
    "    \n",
    "    # create a minute index with all the minutes in the sleep episode\n",
    "    start = json_sleep_episode['startTime']\n",
    "    end = json_sleep_episode['endTime']\n",
    "    TIME_INDEX = pd.date_range(start=start,end=end,freq='min')[:-1] # [:-1] b/c json excludes end point\n",
    "    \n",
    "    #YSS\n",
    "    if len(TIME_INDEX) != json_df.shape[0]:\n",
    "        # it is expected that there is a one-to-one matching between dateTime values in minuteData \n",
    "        # and the time index generated between start and end times\n",
    "        print('\\t\\tinvestigate mismatch in timing of sleep data')\n",
    "    \n",
    "    # build final dataframe\n",
    "    df = pd.DataFrame(index=TIME_INDEX)\n",
    "    df['dt'] = df.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['time'] = df['dt'].apply(lambda x: x.split()[1])\n",
    "    df = df.merge(json_df, how='left', left_on='time', right_on='dateTime')\n",
    "    df = df[['dt', 'value']]\n",
    "    df = df.rename(columns={'dt':'dateTime'})\n",
    "    \n",
    "    #YSS\n",
    "    if len(TIME_INDEX) != df.shape[0]:\n",
    "        # it is expected that there is a one-to-one matching between dateTime values in minuteData \n",
    "        # and the time index generated between start and end times\n",
    "        print('\\t\\tinvestigate issue in timestamping sleep data')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: json file\n",
    "# output: raw sleep dataframe\n",
    "def getParticipantSleepData(filename):\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # fix JSON formatting errors\n",
    "    count = data[0].count(\"}}\") - 1\n",
    "    data_val = '[' + data[0].replace('}}','}},', count) + ']'\n",
    "\n",
    "    # load JSON data\n",
    "    json_data = json.loads(data_val)\n",
    "    \n",
    "    # initialize empty dataframe\n",
    "    all_df = pd.DataFrame()\n",
    "    #all_df = [] #YSS\n",
    "    \n",
    "    # append all sleep episodes to empty dataframe\n",
    "    for idx, sleep_day in enumerate(json_data):\n",
    "        print('\\tprocessing sleep day {:03}...'.format(idx)) #YSS\n",
    "        for episode in sleep_day['sleep']:\n",
    "            episode_df = getDF(episode)\n",
    "            all_df = all_df.append(episode_df) #YSS use of concatenation is more efficient\n",
    "            \n",
    "    return all_df\n",
    "    #return pd.concat(all_df) #YSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAllJSONToCSV(phase):\n",
    "    \n",
    "    print('Beginning directory conversion...')\n",
    "\n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "\n",
    "    csv_dir = os.path.join(uw, 'daily_sleep_raw_csv') #YSS sleep_raw_csv -> daily_sleep_raw_csv\n",
    "    json_dir = os.path.join(uw, 'daily_sleep_raw_json')        \n",
    "        \n",
    "    for fn in tqdm(os.listdir(json_dir)):\n",
    "        print('converting {}...'.format(fn)) #YSS\n",
    "        df = getParticipantSleepData(os.path.join(json_dir, fn))\n",
    "        new_fn = os.path.join(csv_dir,fn.replace('.json', '.csv'))\n",
    "        df.to_csv(new_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "convertAllJSONToCSV('uw1')\n",
    "convertAllJSONToCSV('uw2')\n",
    "#YSS no further investigation was reported (good!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert steps csv to correct format\n",
    "# columns: datetime (%Y-%m-%d %H:%M:%S) and steps\n",
    "def processStepsFile(src, dest):\n",
    "    df = pd.read_csv(src)\n",
    "    df['datetime'] = df.date.str.cat(df.time, sep=\" \")\n",
    "    new_df = df[['datetime', 'steps']]\n",
    "    new_df.to_csv(dest, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entire steps raw folder\n",
    "def processAllSteps(phase):\n",
    "    \n",
    "    print('Beginning directory conversion...')\n",
    "    \n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "\n",
    "\n",
    "    steps_all_dir = os.path.join(uw, 'Daily Step Details')\n",
    "    steps_dir = os.path.join(uw, 'daily_steps_raw_data') #YSS steps_raw_data -> daily_steps_raw_data\n",
    "    \n",
    "    if not os.path.exists(steps_dir): \n",
    "        os.makedirs(steps_dir)    \n",
    "    \n",
    "    for fn in tqdm(os.listdir(steps_all_dir)):\n",
    "        if fn[-3:] == 'csv': \n",
    "            src = os.path.join(steps_all_dir, fn)\n",
    "            dest = os.path.join(steps_dir, fn)\n",
    "            processStepsFile(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "processAllSteps('uw1')\n",
    "processAllSteps('uw2')\n",
    "#YSS NOTE use of detailed step data is potentially problematic because of missing data otherwise available in daily step data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames files from 'PID001_step.csv' -> 'PID001.csv'\n",
    "def convertAllFilenamesToID(phase):\n",
    "    if phase == 'uw1':\n",
    "        uw = 'Fitbit UWEXPI/'\n",
    "    elif phase == 'uw2':\n",
    "        uw = 'Fitbit UWEXPII/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "    \n",
    "    sleep_dir = os.path.join(uw,'daily_sleep_raw_csv/') #YSS sleep_raw_data --> daily_sleep_raw_csv\n",
    "    steps_dir = os.path.join(uw,'daily_steps_raw_data/') #YSS steps_raw_data -> daily_steps_raw_data\n",
    "    \n",
    "    sleep_files = os.listdir(sleep_dir)\n",
    "    steps_files = os.listdir(steps_dir)\n",
    "    \n",
    "    #YSS\n",
    "    sleep_fns_unique = set([fn[:6] for fn in sleep_files])\n",
    "    if len(sleep_fns_unique) != len(sleep_files):\n",
    "        print('investigate split sleep data being overwritten')\n",
    "    step_fns_unique = set([fn[:6] for fn in steps_files])\n",
    "    if len(step_fns_unique) != len(steps_files):\n",
    "        print('investigate split step data being overwritten')\n",
    "    \n",
    "    for f in sleep_files:\n",
    "        src = os.path.join(sleep_dir,f)\n",
    "        new_f = f[:6] +'.csv'\n",
    "        dest = os.path.join(sleep_dir, new_f)\n",
    "        os.rename(src,dest)\n",
    "        \n",
    "    for f in steps_files:\n",
    "        src = os.path.join(steps_dir,f)\n",
    "        new_f = f[:6] +'.csv'\n",
    "        dest = os.path.join(steps_dir, new_f)\n",
    "        os.rename(src,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "convertAllFilenamesToID('uw1')\n",
    "convertAllFilenamesToID('uw2')\n",
    "#YSS PID136 data is split and the second one overwrited the first one in both sleep and step data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Of UW-Specific Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have multiple datasets, and getPath allows us to grab the path of whatever dataset we're working with. Each dataset folder assumes a similar underlying directory structure:\n",
    "* steps_raw_data\n",
    "* sleep_raw_data\n",
    "* sleep_steps_data\n",
    "* sleep_episodes\n",
    "* computed_features\n",
    "\n",
    "The steps_raw_data is optional, and for the NetHealth dataset, sleep_steps_data is missing steps. We don't need steps to help us compute sleep, rather steps is useful in differentiating missing data from all-nighters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPath(phase):\n",
    "    if phase == 'uw1':\n",
    "        path = 'Fitbit UWEXPI/'    \n",
    "    elif phase == 'uw2':\n",
    "        path = 'Fitbit UWEXPII/'\n",
    "    elif phase == 'lac1':\n",
    "        path = 'LifeAtCMU_Phase1/'\n",
    "    elif phase == 'lac2':\n",
    "        path = 'LifeAtCMU_Phase2/'\n",
    "    elif phase == 'nh':\n",
    "        path = 'NetHealth/'\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSleepStepsDir():\n",
    "    for d in ['uw1','uw2','lac1','lac2']:\n",
    "        new_d = os.path.join(getPath(d),'sleep_steps_data/')\n",
    "        if not os.path.exists(new_d): \n",
    "            os.makedirs(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "createSleepStepsDir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following combines all sleep and steps into a single combined dataframe for the given date range in the sleep_steps_data folder. This step is optional for these sleep analyses, and is done for NetHealth in the NetHealth cleaning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines all raw sleep and raw steps \n",
    "# input: \"uw1\", \"uw2\", \"lac1\", \"lac2\"\n",
    "def saveAllCombined(phase):\n",
    "\n",
    "    if phase == 'uw1':\n",
    "        TIME_INDEX = pd.date_range(start='1/1/2018',\n",
    "                                   end='6/13/2018',\n",
    "                                   freq='min')\n",
    "    elif phase == 'uw2': \n",
    "        TIME_INDEX = pd.date_range(start='4/1/2019',\n",
    "                                   end='6/14/2019',\n",
    "                                   freq='min')\n",
    "    elif phase == 'lac1':\n",
    "        TIME_INDEX = pd.date_range(start='1/16/2017',\n",
    "                                   end='5/15/2017',\n",
    "                                   freq='min')\n",
    "    elif phase == 'lac2':\n",
    "        TIME_INDEX = pd.date_range(start='1/16/2018',\n",
    "                                   end='5/15/2018',\n",
    "                                   freq='min')\n",
    "    else:\n",
    "        raise('Invalid phase argument')\n",
    "        \n",
    "    path = getPath(phase)\n",
    "    \n",
    "    \n",
    "    \n",
    "    sleep_dir = os.path.join(path,'daily_sleep_raw_csv/') #YSS sleep_raw_data --> daily_sleep_raw_data\n",
    "    steps_dir = os.path.join(path,'daily_steps_raw_data/') #YSS steps_raw_data -> daily_steps_raw_data\n",
    "    combined_dir = os.path.join(path,'sleep_steps_data/')\n",
    "\n",
    "    sleep_files = os.listdir(sleep_dir)\n",
    "    steps_files = os.listdir(steps_dir)\n",
    "    both = list(set(sleep_files) & set(steps_files))\n",
    "    both.sort() #YSS\n",
    "\n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    bad_ids = []\n",
    "    for idx in tqdm(range(len(both))):\n",
    "        \n",
    "        #print('check if {} exists ...'.format(os.path.join(combined_dir, both[idx])), end=' ') #YSS\n",
    "\n",
    "        if os.path.exists(os.path.join(combined_dir, both[idx])):\n",
    "            #print('YES; continue to the next file') #YSS\n",
    "            continue\n",
    "        else:\n",
    "            #print('NO; combine sleep and step files') #YSS\n",
    "            try:\n",
    "                df_sleep = pd.read_csv(os.path.join(sleep_dir, both[idx]))\n",
    "                df_steps = pd.read_csv(os.path.join(steps_dir, both[idx]))\n",
    "\n",
    "                #YSS - check if all rows have consistent second values\n",
    "                secs = df_sleep['dateTime'].apply(lambda time: time[-2:]).unique()\n",
    "                if len(secs) > 1:\n",
    "                    print('inconsistent sleep second values for {}: {}'.format(both[idx], list(secs)))\n",
    "                secs = df_steps['datetime'].apply(lambda time: time[-2:]).unique()\n",
    "                if len(secs) > 1:\n",
    "                    print('inconsistent step second values for {}: {}'.format(both[idx], list(secs)))\n",
    "                # NOTE should there be inconsistentsies, there is a chance that replacing seconds with 0 results in \n",
    "                #      duplicate timestamps\n",
    "                #print('finished 1st check')\n",
    "\n",
    "\n",
    "                # Convert times to datetime objects\n",
    "                df_sleep['dt_typed'] = df_sleep['dateTime'].apply(lambda time: datetime.strptime(time,DATE_FORMAT).replace(second=0))\n",
    "                df_steps['dt_typed'] = df_steps['datetime'].apply(lambda time: datetime.strptime(time,DATE_FORMAT).replace(second=0))\n",
    "                df_sleep = df_sleep.rename(columns={'value' : 'sleep_value'})\n",
    "\n",
    "                #YSS - check if any rows have date&time values that do not appear in TIME_INDEX\n",
    "                dt_index = df_sleep['dt_typed'].isin(TIME_INDEX)\n",
    "                if (~dt_index).sum() > 0:\n",
    "                    print('inconsistent sleep timestamps for {}: {}'.format(both[idx], df_sleep[~dt_index]['dt_typed']))\n",
    "                dt_index = df_steps['dt_typed'].isin(TIME_INDEX)\n",
    "                if (~dt_index).sum() > 0:\n",
    "                    print('inconsistent step timestamps for {}: {}'.format(both[idx], df_steps[~dt_index]['dt_typed']))\n",
    "                # NOTE if all timestamps exist in TIME_INDEX there should not be any duplicates\n",
    "                #print('finished 2nd check')\n",
    "\n",
    "                #YSS - check if there are duplicate timestamps\n",
    "                duplicates = df_sleep.groupby(by=['dt_typed']).size()\n",
    "                if duplicates[duplicates > 1].shape[0] > 0:\n",
    "                    print('duplicates in sleep dataframe for {}: {}'.format(both[idx], duplicates[duplicates > 1]))\n",
    "                duplicates = df_steps.groupby(by=['dt_typed']).size()\n",
    "                if duplicates[duplicates > 1].shape[0] > 0:\n",
    "                    print('duplicates in step dataframe for {}'.format(both[idx], duplicates[duplicates > 1]))\n",
    "                # NOTE duplciates in combination can only exist if there are duplciates in the original timestamps\n",
    "                #print('finished 3rd check')\n",
    "\n",
    "                df = pd.DataFrame(index=TIME_INDEX)\n",
    "                df = df.merge(df_sleep[['sleep_value','dt_typed']],how='left',left_index=True,right_on='dt_typed')\n",
    "                df = df.merge(df_steps[['steps','dt_typed']],how='left',left_on='dt_typed',right_on='dt_typed')\n",
    "\n",
    "                #YSS\n",
    "                duplicates = df.groupby(by=['dt_typed']).size()\n",
    "                if duplicates[duplicates > 1].shape[0] > 0:\n",
    "                    print('duplicates in combined dataframe for {}'.format(both[idx]))\n",
    "                    duplicates[duplicates > 1].to_csv('debug{}-combine_sleep_step.csv'.format(both[idx][:-4]))\n",
    "                #print('finished 4th check')\n",
    "\n",
    "                df = df.drop_duplicates(subset='dt_typed', keep='first') #YSS why should there be any duplicates?\n",
    "                df = df.set_index('dt_typed')\n",
    "\n",
    "                # lac1 only gives steps every 5 minutes, so we interpolate\n",
    "                if phase == 'lac1':\n",
    "                    df['steps'] = df['steps'].interpolate()\n",
    "\n",
    "                df.index.names = ['time']\n",
    "                df.sort_index(inplace=True)\n",
    "                df.to_csv(os.path.join(combined_dir, both[idx]))\n",
    "            except:\n",
    "                print('exception happened in processing', sys.exc_info()) #YSS\n",
    "                bad_ids.append(both[idx])\n",
    "    print(phase, bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN - uw1/2 both have a few empty participants\n",
    "# these will be ignored and their PIDs will be printed\n",
    "\n",
    "saveAllCombined('uw1')\n",
    "saveAllCombined('uw2')\n",
    "#YSS no duplicates for UW-I or UW-II\n",
    "saveAllCombined('lac1')\n",
    "#YSS no duplicates for CMU-I\n",
    "saveAllCombined('lac2')\n",
    "#YSS it seems that data appears twice under '202.csv', '224.csv', '245.csv', '277.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the number of minutes that separates two datetimes\n",
    "def diff_min(dt1,dt2):\n",
    "    \n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    td = datetime.strptime(dt1, DATE_FORMAT) - datetime.strptime(dt2, DATE_FORMAT)\n",
    "    return int(td.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the combined sleep-steps files, we then want to extract the sleep episodes. To do this, we have two parameters: MIN_CONSEC_NON_AWAKE and MAX_CONSEC_AWAKE.\n",
    "\n",
    "MIN_CONSEC_NON_AWAKE is the number of consecutive minutes that a student has to be asleep/restless for an episode to be recognized, and MAX_CONSEC_AWAKE is the number of consecutive awake minutes that have to be recognized for the termination of an episode. Any timepoint where steps are positive is considered awake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CONSEC_NON_AWAKE = 20\n",
    "MAX_CONSEC_AWAKE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectSleepEpisodes(phase, \n",
    "                            filename,\n",
    "                            min_consec_non_awake = MIN_CONSEC_NON_AWAKE,\n",
    "                            max_consec_awake = MAX_CONSEC_AWAKE):\n",
    "        \n",
    "    path = getPath(phase)\n",
    "\n",
    "    COMBINED_DIR = os.path.join(path, 'sleep_steps_data/')\n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')\n",
    "    \n",
    "    # Read combined data\n",
    "    filename = os.path.join(COMBINED_DIR, filename)\n",
    "    print('episodes for', filename) #YSS\n",
    "    combined_df = pd.read_csv(filename, index_col=0)\n",
    "    \n",
    "    #YSS\n",
    "    duplicates = combined_df.groupby(combined_df.index).size()\n",
    "    if duplicates[duplicates > 1].shape[0] > 0:\n",
    "        print('duplicates in combined dataframe for {}'.format(filename))\n",
    "        duplicates[duplicates > 1].to_csv('debug{}-episode_sleep_step.csv'.formt(filename[:-4]))\n",
    "    \n",
    "    # Define fields\n",
    "    columns = ['start_time', 'end_time', 'length', 'time_asleep', 'time_restless', 'time_awake']\n",
    "    \n",
    "    # Make dictionary\n",
    "    episode_dict = {} # start_time -> ['end_time', 'length', 'time_asleep', 'time_restless', 'time_awake']\n",
    "    \n",
    "    # tracks the last time point the subject was awake\n",
    "    last_awake = combined_df.index[0]\n",
    "    \n",
    "    # used for tracking when in or not in and episode\n",
    "    consec_non_awake = 0\n",
    "    consec_awake = 0\n",
    "    on_episode = False\n",
    "    start_index = None\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep='first')] #YSS what is this about duplicated   \n",
    "    \n",
    "    # iterate through the subject's combined dataframe\n",
    "    for index, row in combined_df.iterrows():\n",
    "        sleep = row['sleep_value']\n",
    "\n",
    "        # update counts\n",
    "        if math.isnan(sleep) or sleep == 3 or sleep == 0: # 3 = awake, 0 = missing in lac1\n",
    "            consec_awake += 1\n",
    "            consec_non_awake = 0\n",
    "            last_awake = index\n",
    "        elif sleep == 1: # 1 = asleep\n",
    "            consec_awake = 0\n",
    "            consec_non_awake += 1\n",
    "        elif sleep == 2: # 2 = restless\n",
    "            consec_awake = 0\n",
    "            consec_non_awake += 1\n",
    "        else:\n",
    "            raise Exception('sleep value is not NaN, 1, 2, or 3')\n",
    "\n",
    "        # update episode status\n",
    "        # starting new episode\n",
    "        if not on_episode and consec_non_awake >= min_consec_non_awake:\n",
    "            on_episode = True\n",
    "            start_index = last_awake\n",
    "            start_loc = combined_df.index.get_loc(index)\n",
    "        # ending episode\n",
    "        elif on_episode and consec_awake >= max_consec_awake:\n",
    "            index_loc = combined_df.index.get_loc(index)\n",
    "            end_index = combined_df.index[index_loc-max_consec_awake+1]\n",
    "            length = diff_min(end_index,start_index)\n",
    "            temp_df = combined_df.iloc[start_loc:index_loc]\n",
    "            time_awake = len(temp_df[temp_df['sleep_value']==3])\n",
    "            time_restless = len(temp_df[temp_df['sleep_value']==2])\n",
    "            time_asleep = length - time_restless - time_awake\n",
    "            episode_dict[start_index] = [end_index, length, time_asleep, time_restless, time_awake]\n",
    "            start_index = index\n",
    "            on_episode = False\n",
    "\n",
    "    # Make dataframe from dictionary\n",
    "    episode_df = pd.DataFrame.from_dict(episode_dict, orient='index', columns=columns[1:])\n",
    "    episode_df.index.name = columns[0]\n",
    "    \n",
    "    return episode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIN_CONSEC_NON_AWAKE = 4\n",
    "#MAX_CONSEC_AWAKE = 2\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([1, 1, 3, 1, 1, 1, 3, 1, 1], index=combined_df.iloc[:9].index) # no episode\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([3, 3, 1, 1, 1, 1, 3, 1, 1], index=combined_df.iloc[:9].index) # no episode\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([1, 3, 1, 1, 1, 1, 3, 3, 1], index=combined_df.iloc[:9].index)\n",
    "#combined_df.loc[combined_df.iloc[:9].index, 'sleep_value'] = pd.Series([3, 1, 1, 1, 1, 3, 1, 3, 3], index=combined_df.iloc[:9].index)\n",
    "#YSS issues in episode calculations (all minor):\n",
    "# - sensitive to awake oscillation at the start of an episode but not in the middle\n",
    "# - neither start_time nor end_time are inclusive \n",
    "#   (the episode actually starts after start_time and ends before end_time)\n",
    "# - actual length of time is smaller by one minute\n",
    "# - cannot find episodes if there are not enough awake minutes at the end of the timeseries; \n",
    "#   can be fixed by padding MAX_CONSEC_AWAKE awake minutes at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAllEpisodesForParams(phase,\n",
    "                                min_consec_non_awake, \n",
    "                                max_consec_awake):\n",
    "\n",
    "    path = getPath(phase)\n",
    "\n",
    "    COMBINED_DIR = os.path.join(path, 'sleep_steps_data/')\n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')    \n",
    "        \n",
    "    if not os.path.exists(EPISODES_DIR): \n",
    "        os.makedirs(EPISODES_DIR) \n",
    "    \n",
    "    for src in tqdm(os.listdir(COMBINED_DIR)):\n",
    "\n",
    "        dest = 'EPI_' + src.split('.')[0] + '.csv'\n",
    "\n",
    "        dest = os.path.join(EPISODES_DIR,dest)\n",
    "\n",
    "        if not os.path.exists(dest):    # remove to writeover existing files\n",
    "            df = getSubjectSleepEpisodes(\n",
    "                     phase,\n",
    "                     src,\n",
    "                     min_consec_non_awake,\n",
    "                     max_consec_awake)\n",
    "            df.to_csv(dest)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN - took 3-5 hours on my personal computer \n",
    "# for phase in ['uw1', 'uw2', 'lac1', 'lac2', 'nh']:\n",
    "#     computeAllEpisodesForParams(phase,\n",
    "#                             MIN_CONSEC_NON_AWAKE,\n",
    "#                             MAX_CONSEC_AWAKE)\n",
    "computeAllEpisodesForParams('uw1', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "computeAllEpisodesForParams('uw2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "computeAllEpisodesForParams('lac1', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "computeAllEpisodesForParams('lac2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS\n",
    "#computeAllEpisodesForParams('nh', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE) #YSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeAllEpisodesForParams('uw2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeAllEpisodesForParams('lac1', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeAllEpisodesForParams('lac2', MIN_CONSEC_NON_AWAKE, MAX_CONSEC_AWAKE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've extracted the sleep episodes (which are in the sleep_episodes/ folder under a subdirectory titled \"MIN_CONSEC_NONAWAKE\"\"MAX_CONSEC_AWAKE\"), we then have to compute the sleep features of interest.\n",
    "\n",
    "To do so, we need to convert the bedtime and waketime into minutes after a given zero (e.g. minutes after 6 pm) since time is modulo 24 hr. For bedtime, students generally go to bed after 6 pm, and for waketime, students generally wake up after 4 am (these numbers were not chosen formally). Thus, we set BED_ZERO = 18 (i.e. 6 pm), and WAKE_ZERO = 4 (i.e. 4 am), and convert bedtime and waketime to the number of minutes after these respective zeros.\n",
    "\n",
    "Additionally, we identified the main sleep episode of Day n as the longest sleep episode which started between noon of Day n and noon of Day (n+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BED_ZERO = 18\n",
    "WAKE_ZERO = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the main episode date for given time\n",
    "def getWindow(time):\n",
    "    if time.hour < 12: #YSS use of 6 is more appropriate; see UW-I PID155 on 2018-06-04\n",
    "        time = time - timedelta(days=1)\n",
    "    return time.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpisodeData(phase,\n",
    "                   filename): # ex: 'PID001.csv'\n",
    "    \n",
    "    DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "    \n",
    "    path = getPath(phase)\n",
    "    \n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')\n",
    "    file = os.path.join(EPISODES_DIR, 'EPI_'+ filename.split('.')[0] + '.csv')\n",
    "    \n",
    "    df = pd.read_csv(file, header=0) #YSS consider using pd.read_csv(file, header=0, parse_dates=['start_time', 'end_time'])\n",
    "    \n",
    "    df['start_time'] = df['start_time'].apply(lambda x: datetime.strptime(x,DATE_FORMAT)) #YSS not needed if using parse_dates in read_csv\n",
    "    df['end_time'] = df['end_time'].apply(lambda x: datetime.strptime(x,DATE_FORMAT)) #YSS not needed if using parse_dates in read_csv\n",
    "    \n",
    "    # define a main sleep episode as the longest sleep episode\n",
    "    # in sleep window\n",
    "    df['subject_id'] = filename.split('.')[0]\n",
    "    df['main_episode_of'] = df['start_time'].apply(lambda x: getWindow(x))\n",
    "    #df = df.sort_values(by='length', ascending=False) #YSS not needed\n",
    "    #df = df.sort_values(by='main_episode_of') #YSS not needed\n",
    "    \n",
    "    #YSS\n",
    "    df.index.name = 'inds'\n",
    "    main_index = df.reset_index().groupby(by=['main_episode_of']).apply(lambda x: \n",
    "                                                                        x.sort_values(by=['length'], \n",
    "                                                                                      ascending=False).iloc[0]['inds'])\n",
    "    df['main_sleep'] = False\n",
    "    df.loc[df.index.isin(main_index), 'main_sleep'] = True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>length</th>\n",
       "      <th>time_asleep</th>\n",
       "      <th>time_restless</th>\n",
       "      <th>time_awake</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>main_episode_of</th>\n",
       "      <th>main_sleep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-05 04:20:00</td>\n",
       "      <td>2018-01-05 09:58:00</td>\n",
       "      <td>338</td>\n",
       "      <td>335</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-06 02:57:00</td>\n",
       "      <td>2018-01-06 08:34:00</td>\n",
       "      <td>337</td>\n",
       "      <td>319</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-07 03:16:00</td>\n",
       "      <td>2018-01-07 06:34:00</td>\n",
       "      <td>198</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-07 06:40:00</td>\n",
       "      <td>2018-01-07 08:48:00</td>\n",
       "      <td>128</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08 00:38:00</td>\n",
       "      <td>2018-01-08 08:30:00</td>\n",
       "      <td>472</td>\n",
       "      <td>460</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2018-06-10 00:26:00</td>\n",
       "      <td>2018-06-10 01:08:00</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2018-06-10 01:13:00</td>\n",
       "      <td>2018-06-10 10:22:00</td>\n",
       "      <td>549</td>\n",
       "      <td>519</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2018-06-10 23:20:00</td>\n",
       "      <td>2018-06-11 07:48:00</td>\n",
       "      <td>508</td>\n",
       "      <td>486</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2018-06-11 08:22:00</td>\n",
       "      <td>2018-06-11 10:10:00</td>\n",
       "      <td>108</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-06-10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2018-06-11 12:22:00</td>\n",
       "      <td>2018-06-11 14:19:00</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PID001</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              start_time            end_time  length  time_asleep  \\\n",
       "inds                                                                \n",
       "0    2018-01-05 04:20:00 2018-01-05 09:58:00     338          335   \n",
       "1    2018-01-06 02:57:00 2018-01-06 08:34:00     337          319   \n",
       "2    2018-01-07 03:16:00 2018-01-07 06:34:00     198          189   \n",
       "3    2018-01-07 06:40:00 2018-01-07 08:48:00     128          126   \n",
       "4    2018-01-08 00:38:00 2018-01-08 08:30:00     472          460   \n",
       "...                  ...                 ...     ...          ...   \n",
       "169  2018-06-10 00:26:00 2018-06-10 01:08:00      42           35   \n",
       "170  2018-06-10 01:13:00 2018-06-10 10:22:00     549          519   \n",
       "171  2018-06-10 23:20:00 2018-06-11 07:48:00     508          486   \n",
       "172  2018-06-11 08:22:00 2018-06-11 10:10:00     108          107   \n",
       "173  2018-06-11 12:22:00 2018-06-11 14:19:00     117          117   \n",
       "\n",
       "      time_restless  time_awake subject_id main_episode_of  main_sleep  \n",
       "inds                                                                    \n",
       "0                 3           0     PID001      2018-01-04        True  \n",
       "1                16           2     PID001      2018-01-05        True  \n",
       "2                 5           4     PID001      2018-01-06        True  \n",
       "3                 2           0     PID001      2018-01-06       False  \n",
       "4                12           0     PID001      2018-01-07        True  \n",
       "...             ...         ...        ...             ...         ...  \n",
       "169               3           4     PID001      2018-06-09       False  \n",
       "170              29           1     PID001      2018-06-09        True  \n",
       "171              12          10     PID001      2018-06-10        True  \n",
       "172               0           1     PID001      2018-06-10       False  \n",
       "173               0           0     PID001      2018-06-11        True  \n",
       "\n",
       "[174 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YSS\n",
    "getEpisodeData('uw1', 'PID001.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainEpisodeData(phase, filename):\n",
    "    \n",
    "    df = getEpisodeData(phase, filename)\n",
    "    df = df.sort_values(by='length', ascending=False)\n",
    "    df = df.drop_duplicates(subset='main_episode_of',keep='first')\n",
    "    df = df.sort_values(by='main_episode_of') \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeToMin(time, zero_hour):\n",
    "    \n",
    "    hour = time.hour\n",
    "    minute = time.minute\n",
    "    \n",
    "    if hour < zero_hour:\n",
    "        hour += 24\n",
    "        \n",
    "    return (hour-zero_hour)*60 + minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compute various sleep features of interest for a given time window.\n",
    "\n",
    "MSSD refers to mean successive squared difference. This is a measure of variability that also takes into account the temporal nature of the data. For instance, the MSSD of [3,4,10] is ((4-3)^2 + (10-4)^2)/2 = 37/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMainEpisodeMSSD(main_episode_df):\n",
    "    \n",
    "    main_episode_df = main_episode_df.sort_values(by='main_episode_of')\n",
    "    \n",
    "    main_episode_df['bedtime'] = main_episode_df['start_time'].apply(lambda x: timeToMin(x,BED_ZERO))\n",
    "    main_episode_df['waketime'] = main_episode_df['end_time'].apply(lambda x: timeToMin(x,WAKE_ZERO))\n",
    "    main_episode_df['midpoint_sleep'] = (main_episode_df['waketime'] + main_episode_df['bedtime']) / 2.0\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    bt_total = []\n",
    "    wt_total = []\n",
    "    mp_total = []\n",
    "    \n",
    "    for i, idx1 in enumerate(main_episode_df.index): #YSS this does not handle data dicontinuity (e.g. missing days)\n",
    "        row1 = main_episode_df.loc[idx1]\n",
    "        \n",
    "        if i < len(main_episode_df.index)-1:\n",
    "            idx2 = main_episode_df.index[i+1]\n",
    "            row2 = main_episode_df.loc[idx2]\n",
    "        else:\n",
    "            row2 = None\n",
    "        \n",
    "        if row2 is not None:\n",
    "            count += 1\n",
    "            bt_total.append((row2['bedtime']-row1['bedtime'])**2)\n",
    "            wt_total.append((row2['waketime']-row1['waketime'])**2)\n",
    "            mp_total.append((row2['midpoint_sleep']-row1['midpoint_sleep'])**2)\n",
    "        \n",
    "    \n",
    "    if count == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    bt_mssd = np.array(bt_total).mean() / (float(count) * 3600) #YR: why deviding by float(count) SP: there should not be\n",
    "    wt_mssd = np.array(wt_total).mean() / (float(count) * 3600)\n",
    "    mp_mssd = np.array(mp_total).mean() / (float(count) * 3600)\n",
    "        \n",
    "    bt_mssd_median = median(bt_total)\n",
    "    wt_mssd_median = median(wt_total)\n",
    "    mp_mssd_median = median(mp_total)\n",
    "        \n",
    "    return bt_mssd, wt_mssd, mp_mssd, bt_mssd_median, wt_mssd_median, mp_mssd_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YSS\n",
    "def mssd(s:pd.Series)->float:\n",
    "    \"\"\"\n",
    "    returns the mean of squared difference of consecutive elements of s.\n",
    "    returns NaN if s is None, empty, or has a single element\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return np.nan\n",
    "    if len(s) == 0:\n",
    "        return np.nan\n",
    "    delta_squared = (s - s.shift(-1))**2\n",
    "    return delta_squared.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YSS\n",
    "def getMainEpisodeMSSD(main_episode_df):\n",
    "    main_episode_df = main_episode_df.sort_values(by='main_episode_of', ascending=True)\n",
    "\n",
    "    main_episode_df['day'] = main_episode_df['main_episode_of'].apply(lambda x: x.timetuple().tm_yday) # day of year\n",
    "    main_episode_df['diff'] = main_episode_df['day'].shift(-1) - main_episode_df['day'] # number of days to the next row\n",
    "    main_episode_df['consec'] = False # am I the consecutive day of my previous row?\n",
    "    main_episode_df.loc[main_episode_df['diff'] == 1, 'consec'] = True\n",
    "    main_episode_df['consec'] = main_episode_df['consec'].shift(1).replace({None:True})\n",
    "    main_episode_df['stretch'] = (~main_episode_df['consec']).cumsum() # episodes are streteches of consecutive days\n",
    "\n",
    "    main_episode_df['bedtime'] = main_episode_df['start_time'].apply(lambda x: timeToMin(x,BED_ZERO))\n",
    "    main_episode_df['waketime'] = main_episode_df['end_time'].apply(lambda x: timeToMin(x,WAKE_ZERO))\n",
    "    main_episode_df['midpoint_sleep'] = (main_episode_df['waketime'] + main_episode_df['bedtime']) / 2.0\n",
    "\n",
    "    bt_mssds = main_episode_df.groupby(by=['stretch']).apply(lambda x: mssd(x['bedtime']))\n",
    "    wt_mssds = main_episode_df.groupby(by=['stretch']).apply(lambda x: mssd(x['waketime']))\n",
    "    mp_mssds = main_episode_df.groupby(by=['stretch']).apply(lambda x: mssd(x['midpoint_sleep']))\n",
    "    # TO-DO further test (1) consecutive day episode construction (single episode-DONE, one-day episode-DONE, every-other days)\n",
    "    #                    (2) mssd calculation (None, empty, single-day episode-DONE)\n",
    "    # outstanding questions: how to compute the total mssd based on mssd in each episode\n",
    "    #                        what's the deal with median measures; a separate function (e.g. mssd_median) should be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YSS\n",
    "file = '/Users/yasaman/UWEXP/cmu-sleep-gpa/Fitbit UWEXPI/sleep_episodes/EPI_PID155.csv'\n",
    "df = pd.read_csv(file, header=0, parse_dates=['start_time', 'end_time'])\n",
    "\n",
    "df['main_episode_of'] = df['start_time'].apply(lambda x: getWindow(x))\n",
    "df.index.name = 'inds'\n",
    "main_index = df.reset_index().groupby(by=['main_episode_of']).apply(lambda x: \n",
    "                                                                    x.sort_values(by=['length'], \n",
    "                                                                                  ascending=False).iloc[0]['inds'])\n",
    "df['main_sleep'] = False\n",
    "df.loc[df.index.isin(main_index), 'main_sleep'] = True\n",
    "\n",
    "main_episode_df = df[df['main_sleep']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given time window, computeSummaryStats computes all the sleep features for a single participant, while computeAllSummaryStats computes all the sleep features for **all** participants for the given time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_window and end_window are date objects\n",
    "# extracts start to end inclusive\n",
    "def computeSummaryStats(phase, \n",
    "                        filename, # ex: 'PID001.csv'\n",
    "                        start_window, \n",
    "                        end_window):\n",
    "    \n",
    "        \n",
    "        DATE_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "        \n",
    "        summary = dict()\n",
    "        \n",
    "        subject_id = filename.split('.')[0]\n",
    "        \n",
    "        summary['subject_id'] = subject_id\n",
    "        \n",
    " \n",
    "        all_df = getEpisodeData(phase, subject_id)\n",
    "        main_df = getMainEpisodeData(phase, subject_id)\n",
    "\n",
    "        start = pd.Timestamp(start_window)\n",
    "        end = pd.Timestamp(end_window)\n",
    "        main_df = main_df[main_df['main_episode_of'].between(start,end)]\n",
    "        \n",
    "        #all_df['start_time'] = all_df['start_time'].apply(lambda x: datetime.strptime(str(x),DATE_FORMAT)) #YSS this is already done in getEpisodeData\n",
    "        #all_df['main_episode_of'] = all_df['start_time'].apply(lambda x: getWindow(x)) #YSS this s already done in getEpisodeData\n",
    "        all_df = all_df[all_df['main_episode_of'].between(start,end)] \n",
    "        \n",
    "        nap_df = all_df[~all_df.start_time.isin(main_df.start_time)]\n",
    "        \n",
    "        #YSS\n",
    "        if all_df.shape[0] != (main_df.shape[0] + nap_df.shape[0]):\n",
    "            print('investigate construction of nap_df in {} (inequality)'.format(filename))\n",
    "        if all_df[~all_df['main_sleep']].shape[0] != nap_df.shape[0]:\n",
    "            print('investigate construction of nap_df in {} (alternative construction)'.format(filename))\n",
    "        \n",
    "        summary['num_naps'] = len(nap_df)\n",
    "        summary['avg_nap_length'] = nap_df['length'].mean()\n",
    "        \n",
    "        #YSS\n",
    "        if all_df.shape[0] > 0:\n",
    "            if all_df[~all_df['main_sleep']]['length'].mean() != summary['avg_nap_length']:\n",
    "                print('investigate construction of nap_df {} (alternative construction - mean)'.format(filename))\n",
    "            if all_df[~all_df['main_sleep']]['length'].sum() != nap_df['length'].sum():\n",
    "                print('investigate construction of nap_df {} (alternative construction - sum)'.format(filename))\n",
    "            if abs((all_df.groupby(all_df.set_index('start_time').index.dayofyear)['length'].sum()).mean() - \n",
    "                all_df['length'].sum() / len(all_df['main_episode_of'].unique())) > 0.01:\n",
    "                print('investigate construction of nap_df {} (alternative construction - sum)'.format(filename))\n",
    "        \n",
    "        try:\n",
    "            summary['frac_napped_of_total_sleep'] = nap_df['length'].sum() / all_df['length'].sum()\n",
    "            summary['avg_24_hr_sleep'] = all_df['length'].sum() / len(all_df['main_episode_of'].unique()) \n",
    "            summary['frac_sleep_episodes_as_naps'] = len(nap_df) / len(all_df)\n",
    "            \n",
    "        except: # all_df is empty\n",
    "            #YSS suggest filling in all the other feilds and returning summary\n",
    "            summary['frac_napped_of_total_sleep'] = np.nan\n",
    "            summary['avg_24_hr_sleep'] = np.nan\n",
    "            summary['frac_sleep_episodes_as_naps'] = np.nan\n",
    "        \n",
    "        summary['frac_nights_with_data'] = len(main_df) / ((end_window-start_window).days + 1)\n",
    " \n",
    "\n",
    "        if len(main_df) == 0:\n",
    "            #YSS this is not consistent with try-catch above\n",
    "            return None\n",
    "\n",
    "        \n",
    "        main_df['bedtime'] = main_df['start_time'].apply(lambda x: timeToMin(x,BED_ZERO))\n",
    "        main_df['waketime'] = main_df['end_time'].apply(lambda x: timeToMin(x,WAKE_ZERO))\n",
    "        main_df['midpoint_sleep'] = (main_df['waketime'] + main_df['bedtime']) / 2.0 #YSS waketime and bedtime are calculated wrt different reference points; what does it mean to find their average? what reference point to use to interpret this average?\n",
    "        #YSS alterantive approach find mid_time based on 'start_time' and length of the main episode then similarly remap it wrt a reference (e.g. BED_ZERO)\n",
    "\n",
    "            \n",
    "        main_df['proportion_awake'] = main_df['time_awake'] / main_df['length']\n",
    "        main_df['proportion_restless'] = main_df['time_restless'] / main_df['length']\n",
    "        main_df['weekday'] = main_df['main_episode_of'].apply(lambda x: x.weekday())\n",
    "        \n",
    "        # 4 = friday night, 5 = saturday night\n",
    "        main_df['is_weekend'] = main_df['weekday'].apply(lambda x: x in [4,5])\n",
    "        \n",
    "        # bedtime measures\n",
    "        summary['bedtime'] = main_df['bedtime'].mean()\n",
    "        summary['bedtime_std'] = main_df['bedtime'].std()\n",
    "    \n",
    "        # waketime measures\n",
    "        summary['waketime'] = main_df['waketime'].mean()\n",
    "        \n",
    "        # midpoint sleep measures\n",
    "        summary['midpoint_sleep'] = main_df['midpoint_sleep'].mean()\n",
    "\n",
    "        # weekday measures\n",
    "        try:  \n",
    "            summary['bedtime_weekday'] = main_df.groupby('is_weekend')['bedtime'].mean()[False]        \n",
    "            summary['waketime_weekday'] = main_df.groupby('is_weekend')['waketime'].mean()[False]\n",
    "            summary['midpoint_sleep_weekday'] = main_df.groupby('is_weekend')['midpoint_sleep'].mean()[False]            \n",
    "        except KeyError: #YSS if an empty summary (all fields are np.nan) is returned in the first try-catch this won't be needed\n",
    "            summary['bedtime_weekday'] = np.nan     \n",
    "            summary['waketime_weekday'] = np.nan\n",
    "            summary['midpoint_sleep_weekday'] = np.nan                \n",
    "        \n",
    "        # weekend measures\n",
    "        try:\n",
    "            summary['bedtime_weekend'] = main_df.groupby('is_weekend')['bedtime'].mean()[True]  \n",
    "            summary['waketime_weekend'] = main_df.groupby('is_weekend')['waketime'].mean()[True]      \n",
    "            summary['midpoint_sleep_weekend'] = main_df.groupby('is_weekend')['midpoint_sleep'].mean()[True]      \n",
    "        except KeyError: #YSS if an empty summary (all fields are np.nan) is returned in the first try-catch this won't be needed\n",
    "            summary['bedtime_weekend'] = np.nan\n",
    "            summary['waketime_weekend'] = np.nan\n",
    "            summary['midpoint_sleep_weekend'] = np.nan\n",
    "\n",
    "            \n",
    "        try:\n",
    "            summary['social_jetlag'] = summary['bedtime_weekend'] - summary['bedtime_weekday'] #YSS this is incorrect; midpoint sleep is used for calculating social jetlag\n",
    "        except:\n",
    "            summary['social_jetlag'] = np.nan\n",
    "            \n",
    "        summary['time_in_bed'] = main_df['length'].mean()\n",
    "                \n",
    "        bt_mssd, wt_mssd, mp_mssd, bt_mssd_median, wt_mssd_median, mp_mssd_median = getMainEpisodeMSSD(main_df)\n",
    "        \n",
    "        summary['bedtime_mssd'] = bt_mssd\n",
    "        summary['waketime_mssd'] = wt_mssd\n",
    "        summary['midpoint_sleep_mssd'] = mp_mssd\n",
    "        \n",
    "        summary['bedtime_mssd_median'] = bt_mssd_median\n",
    "        summary['waketime_mssd_median'] = wt_mssd_median\n",
    "        summary['midpoint_sleep_mssd_median'] = mp_mssd_median\n",
    "        \n",
    "        summary['WASO_fraction'] = main_df['proportion_awake'].mean() #YSS what is WASO?\n",
    "        summary['restless_fraction'] = main_df['proportion_restless'].mean()\n",
    "        summary['TST'] = main_df['time_asleep'].mean()\n",
    "        summary['TST_std'] = main_df['time_asleep'].std()\n",
    "        \n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df PID001.csv (alternative construction - sum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subject_id': 'PID001',\n",
       " 'num_naps': 9,\n",
       " 'avg_nap_length': 122.11111111111111,\n",
       " 'frac_napped_of_total_sleep': 0.08573880480574192,\n",
       " 'avg_24_hr_sleep': 512.72,\n",
       " 'frac_sleep_episodes_as_naps': 0.2647058823529412,\n",
       " 'frac_nights_with_data': 0.8928571428571429,\n",
       " 'bedtime': 491.4,\n",
       " 'bedtime_std': 128.23091150992673,\n",
       " 'waketime': 360.16,\n",
       " 'midpoint_sleep': 425.78,\n",
       " 'bedtime_weekday': 453.8888888888889,\n",
       " 'waketime_weekday': 352.05555555555554,\n",
       " 'midpoint_sleep_weekday': 402.97222222222223,\n",
       " 'bedtime_weekend': 587.8571428571429,\n",
       " 'waketime_weekend': 381.0,\n",
       " 'midpoint_sleep_weekend': 484.42857142857144,\n",
       " 'social_jetlag': 133.96825396825398,\n",
       " 'time_in_bed': 468.76,\n",
       " 'bedtime_mssd': 0.3211829668209876,\n",
       " 'waketime_mssd': 0.1348070987654321,\n",
       " 'midpoint_sleep_mssd': 0.14321168499228396,\n",
       " 'bedtime_mssd_median': 12902.5,\n",
       " 'waketime_mssd_median': 3662.5,\n",
       " 'midpoint_sleep_mssd_median': 4949.125,\n",
       " 'WASO_fraction': 0.004501156732832872,\n",
       " 'restless_fraction': 0.044838352985259074,\n",
       " 'TST': 445.24,\n",
       " 'TST_std': 109.18382053521789}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YSS\n",
    "computeSummaryStats('uw1', 'PID001.csv', date(2018,4,1), date(2018,4,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_window and end_window are date objects\n",
    "def computeAllSummaryStats(phase,\n",
    "                           start_window, \n",
    "                           end_window,\n",
    "                           overwrite=False):\n",
    "    \n",
    "    path = getPath(phase)\n",
    "\n",
    "    FEATURES_FOLDER = os.path.join(path, 'computed_features/')\n",
    "    EPISODES_DIR = os.path.join(path, 'sleep_episodes/')\n",
    "    feature_file = str(start_window) + '_' + str(end_window) + '.csv'       \n",
    "    feature_file = os.path.join(FEATURES_FOLDER,feature_file)    \n",
    "   \n",
    "    if os.path.exists(feature_file) and not overwrite:\n",
    "        return 'Already written'\n",
    "        \n",
    "    ids = os.listdir(os.path.join(path, 'sleep_steps_data/'))\n",
    "\n",
    "    \n",
    "    with open(feature_file, 'w') as f:\n",
    "    \n",
    "        header = ['subject_id',\n",
    "                  'num_naps',\n",
    "                  'avg_nap_length',\n",
    "                  'frac_napped_of_total_sleep',\n",
    "                  'avg_24_hr_sleep',\n",
    "                  'frac_sleep_episodes_as_naps',\n",
    "                  'frac_nights_with_data',\n",
    "                  'bedtime',\n",
    "                  'waketime',\n",
    "                  'midpoint_sleep',\n",
    "                  'time_in_bed',\n",
    "                  'bedtime_mssd',\n",
    "                  'bedtime_mssd_median',\n",
    "                  'bedtime_std',\n",
    "                  'bedtime_weekend',\n",
    "                  'bedtime_weekday',\n",
    "                  'waketime_mssd',\n",
    "                  'waketime_mssd_median',\n",
    "                  'waketime_weekend',\n",
    "                  'waketime_weekday',\n",
    "                  'midpoint_sleep_mssd',\n",
    "                  'midpoint_sleep_mssd_median',\n",
    "                  'midpoint_sleep_weekend',\n",
    "                  'midpoint_sleep_weekday',\n",
    "                  'WASO_fraction',\n",
    "                  'restless_fraction',\n",
    "                  'social_jetlag',\n",
    "                  'TST',\n",
    "                  'TST_std']\n",
    "        \n",
    "        w = csv.DictWriter(f, header)\n",
    "        w.writeheader()\n",
    "        \n",
    "        for filename in ids:\n",
    "    \n",
    "            summary = computeSummaryStats(phase, \n",
    "                                          filename,\n",
    "                                          start_window, \n",
    "                                          end_window)\n",
    "                \n",
    "            if not summary:\n",
    "                summary = dict()\n",
    "                for val in header:\n",
    "                    summary[val] = np.nan\n",
    "                summary['subject_id'] = filename.split('.')[0]\n",
    "                summary['frac_nights_with_data'] = 0\n",
    "                \n",
    "            w.writerow(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Analyses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "for d in ['uw1','uw2','lac1','lac2']:#YSS['uw1','uw2','lac1','lac2', 'nh']:\n",
    "    new_d = os.path.join(getPath(d),'computed_features/')\n",
    "    if not os.path.exists(new_d): \n",
    "        os.makedirs(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGRESSION_PERIODS = [('uw1',date(2018,4,1),date(2018,4,28)),\n",
    "                      ('uw2',date(2019,4,7),date(2019,5,4)),\n",
    "                      ('lac1',date(2017,2,7),date(2017,3,7)),\n",
    "                      ('lac2',date(2018,2,7),date(2018,3,7)),\n",
    "                      #YSS('nh', date(2016,2,4), date(2016,3,4))\n",
    "                     ]\n",
    "\n",
    "PREDICTORS = ['bedtime_mssd',\n",
    "              'TST',\n",
    "              'midpoint_sleep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating features for... uw1\n",
      "investigate construction of nap_df PID076.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID062.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID089.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID117.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID103.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID088.csv (alternative construction - sum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8d8c54591c01>:51: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  summary['frac_napped_of_total_sleep'] = nap_df['length'].sum() / all_df['length'].sum()\n",
      "<ipython-input-10-8d8c54591c01>:52: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  summary['avg_24_hr_sleep'] = all_df['length'].sum() / len(all_df['main_episode_of'].unique())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df PID061.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID075.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID115.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID115.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID128.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID048.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID060.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID058.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID064.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID138.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID138.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID104.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID111.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID059.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID073.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID073.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID098.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID113.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID066.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID066.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID015.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID001.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID029.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID029.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID175.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID175.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID149.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID174.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID202.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID028.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID014.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID002.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID162.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID188.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID188.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID163.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID003.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID013.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID205.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID173.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID198.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID199.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID166.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID172.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID012.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID006.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID010.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID004.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID206.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID158.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID170.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID171.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID005.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID011.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID034.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID020.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID008.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID140.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID140.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID154.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID196.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID169.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID169.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID141.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID035.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID023.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID037.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID037.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID209.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID143.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID194.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID194.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID180.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID181.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID195.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID156.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID022.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID026.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID032.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID146.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID191.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID184.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID190.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID153.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID033.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID019.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID031.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID031.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID179.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID193.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID150.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID144.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID024.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID030.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID057.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID043.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID094.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID094.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID080.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID137.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID122.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID095.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID042.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID040.csv (alternative construction - sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df PID068.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID097.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID120.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID109.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID082.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID055.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID045.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID092.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID125.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID130.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID118.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID093.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID087.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID050.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID052.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID046.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID126.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID132.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID127.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID090.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID090.csv (alternative construction - sum)\n",
      "complete!\n",
      "calculating features for... uw2\n",
      "investigate construction of nap_df PID512.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID506.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID300.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID466.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID472.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID314.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID328.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID498.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID498.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID473.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID315.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID301.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID467.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID467.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID507.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID513.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID513.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID505.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID511.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID511.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID317.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID471.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID303.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID458.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID464.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID538.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID510.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID504.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID500.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID474.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID312.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID307.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID313.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID449.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID501.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID501.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID503.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID339.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID305.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID477.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID477.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID488.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID489.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID489.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID310.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID476.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID304.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID338.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID502.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID516.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID571.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID565.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID405.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID363.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID377.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID377.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID411.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID410.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID362.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID566.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID572.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID412.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID374.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID374.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID360.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID406.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID348.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID413.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID375.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID563.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID371.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID403.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID403.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID365.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID364.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID358.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID562.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID560.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID366.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID400.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID414.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID399.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID398.csv (alternative construction - sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df PID415.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID373.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID367.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID429.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID561.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID549.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID424.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID342.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID356.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID430.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID395.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID394.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID380.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID343.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID545.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID551.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID547.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID553.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID355.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID341.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID341.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID427.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID369.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID382.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID368.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID340.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID426.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID432.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID432.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID354.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID546.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID542.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID556.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID378.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID422.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID344.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID393.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID423.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID345.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID557.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID569.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID347.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID421.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID421.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID353.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID390.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID391.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID434.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID352.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID346.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID420.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID408.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID540.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID554.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID533.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID321.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID335.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID484.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID490.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID452.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID334.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID446.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID526.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID524.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID530.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID518.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID336.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID444.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID493.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID493.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID487.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID486.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID492.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID445.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID451.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID525.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID521.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID535.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID469.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID469.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID455.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID333.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID327.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID441.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID496.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID482.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID483.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID440.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID454.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID332.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID508.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID536.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID318.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID324.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID330.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID456.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID456.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID481.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID495.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID494.csv (alternative construction - mean)\n",
      "investigate construction of nap_df PID480.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID457.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID443.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID325.csv (alternative construction - sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df PID319.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID523.csv (alternative construction - sum)\n",
      "investigate construction of nap_df PID537.csv (alternative construction - sum)\n",
      "complete!\n",
      "calculating features for... lac1\n",
      "investigate construction of nap_df 223.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 196.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 357.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 343.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 356.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 197.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 168.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 236.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 222.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 222.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 234.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 220.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 208.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 142.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 181.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 341.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 180.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 180.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 157.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 209.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 231.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 147.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 153.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 153.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 184.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 185.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 152.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 178.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 144.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 144.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 346.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 347.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 179.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 233.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 254.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 254.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 297.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 334.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 309.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 123.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 137.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 282.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 241.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 280.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 109.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 337.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 134.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 120.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 120.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 281.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 281.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 295.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 246.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 291.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 124.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 130.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 130.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 332.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 125.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 125.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 119.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 284.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 286.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 133.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 287.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 287.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 261.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 301.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 300.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 116.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 276.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 289.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 302.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 316.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 303.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 129.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 129.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 288.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 307.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 306.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 110.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 110.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 104.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 299.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 266.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 272.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 258.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 106.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 338.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 310.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 310.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 305.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 265.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 216.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 174.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 174.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 148.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 149.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 149.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 175.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 215.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 215.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 201.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 229.csv (alternative construction - sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df 349.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 189.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 176.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 200.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 214.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 199.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 198.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 173.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 167.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 211.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 171.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 170.csv (alternative construction - sum)\n",
      "complete!\n",
      "calculating features for... lac2\n",
      "investigate construction of nap_df 551.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 586.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 343.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 419.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 380.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 342.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 432.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 383.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 396.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 427.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 209.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 379.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 350.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 422.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 378.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 581.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 595.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 224.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 218.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 540.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 554.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 352.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 391.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 390.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 555.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 541.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 240.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 268.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 283.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 644.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 693.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 452.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 309.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 321.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 453.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 686.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 296.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 255.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 257.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 525.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 653.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 337.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 479.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 487.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 336.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 281.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 518.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 252.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 246.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 326.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 454.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 483.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 497.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 333.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 284.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 290.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 523.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 245.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 319.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 442.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 330.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 318.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 293.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 250.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 278.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 261.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 507.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 507.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 513.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 275.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 249.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 671.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 665.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 315.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 473.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 329.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 466.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 300.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 248.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 512.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 260.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 506.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 276.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 262.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 538.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 303.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 465.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 698.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 263.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 277.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 515.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 461.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 312.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 474.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 460.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 306.csv (alternative construction - sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigate construction of nap_df 662.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 266.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 266.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 264.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 338.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 476.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 304.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 305.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 477.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 477.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 271.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 202.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 559.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 217.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 215.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 413.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 374.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 374.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 412.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 360.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 228.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 200.csv (alternative construction - mean)\n",
      "investigate construction of nap_df 572.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 214.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 204.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 402.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 615.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 563.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 205.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 577.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 239.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 213.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 617.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 373.csv (alternative construction - sum)\n",
      "investigate construction of nap_df 206.csv (alternative construction - sum)\n",
      "complete!\n",
      "calculating features for... nh\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NetHealth/sleep_steps_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9d4426795920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mREGRESSION_PERIODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'calculating features for...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcomputeAllSummaryStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ADD overwrite=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-98b10d763843>\u001b[0m in \u001b[0;36mcomputeAllSummaryStats\u001b[0;34m(phase, start_window, end_window, overwrite)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'Already written'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sleep_steps_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NetHealth/sleep_steps_data/'"
     ]
    }
   ],
   "source": [
    "#YSS\n",
    "for phase, start, end in REGRESSION_PERIODS:\n",
    "    print('calculating features for...', phase)\n",
    "    computeAllSummaryStats(phase, start, end, overwrite=True) # ADD overwrite=True\n",
    "    print('complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAllFeatureWindows():\n",
    "    for period in REGRESSION_PERIODS:\n",
    "\n",
    "        phase = period[0]\n",
    "        start_window = period[1]\n",
    "        end_window = period[2]\n",
    "\n",
    "        computeAllSummaryStats(phase, start_window, end_window, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStats(phase, start, end):\n",
    "\n",
    "    start_window = start\n",
    "    end_window = end\n",
    "    \n",
    "    path = getPath(phase)\n",
    "            \n",
    "    # get entire semester participant data   \n",
    "    filename = str(start_window) + '_' + str(end_window) + '.csv'\n",
    "    filename = os.path.join(path, 'computed_features/', filename)\n",
    "    summary_stats_df = pd.read_csv(filename)\n",
    "    summary_stats_df.set_index('subject_id', inplace=True)\n",
    "    \n",
    "    return summary_stats_df[summary_stats_df['frac_nights_with_data']>=.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to extract first-year student label from EMA for lac2\n",
    "def getLAC2Freshmen():\n",
    "    phase = 'lac2'\n",
    "    path = os.path.join('GPA_data/raw_gpa/',\n",
    "                        phase + '_pre_post_ema.csv')\n",
    "    df = pd.read_csv(path, index_col='ID', low_memory=False)\n",
    "    return list(df[df['IRA_YearOfStudy']==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGPAData(phase):\n",
    "    path = os.path.join('GPA_data/cleaned_gpa/', \n",
    "                        phase + '_freshmen_gpa.csv')\n",
    "    return pd.read_csv(path, index_col='subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all predictors of interest, GPA features, no nan rows,\n",
    "# and participants with >=20% fraction of nights data \n",
    "def getRegDF(phase, start, end, predictors, control=None, thresh=0.2):\n",
    "        \n",
    "        # compute summary statistics\n",
    "        #computeAllSummaryStats(phase, start, end, overwrite=True) # ADD overwrite=True #YSS\n",
    "        stats_df = getSummaryStats(phase, start, end)\n",
    "        \n",
    "        # filter by freshmen\n",
    "        if phase == 'lac2':\n",
    "            stats_df = stats_df[stats_df.index.isin(getLAC2Freshmen())]\n",
    "\n",
    "        # filter by completeness threshold\n",
    "        stats_df = stats_df[stats_df['frac_nights_with_data'] >= thresh] #YSS how is this different from what is alreay done in getSummaryStats?\n",
    "\n",
    "        # get GPA data\n",
    "        gpa_df = getGPAData(phase)\n",
    "        \n",
    "        # combine summary statistics with GPA data\n",
    "        combined_df = stats_df.merge(gpa_df, on='subject_id', how='outer')\n",
    "\n",
    "        # choose only columns of variables of interest\n",
    "        columns = predictors + ['cum_gpa', 'term_gpa', 'frac_nights_with_data']\n",
    "                \n",
    "        if control:\n",
    "            columns += [control]        \n",
    "        \n",
    "        combined_df = combined_df[columns].dropna()\n",
    "        \n",
    "        return combined_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveRegressionDF():\n",
    "\n",
    "    dfs = []\n",
    "    for period in REGRESSION_PERIODS:\n",
    "        df = getRegDF(period[0], period[1], period[2], PREDICTORS, thresh=0.2)\n",
    "        df['cohort'] = period[0]\n",
    "        dfs.append(df)\n",
    "    all_reg_dfs = pd.concat(dfs)\n",
    "    all_reg_dfs.to_csv('regression_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT TO RUN\n",
    "saveRegressionDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
